{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stopped-ozone",
   "metadata": {},
   "source": [
    "# Probability-based Clustering\n",
    "\n",
    "So far, the clustering methods that we have discussed results in hard clutering or partitioning: each point belongs to only one cluster. In this notebook, we will look at algorithms that perform soft clustering: a point's membership to each cluster is described by a certain probability or weighting.\n",
    "\n",
    "## EM: Expectation-Maximization Algorithm\n",
    "\n",
    "The general approach for probability-based clustering is to assume that clusters have a certain characteristic defined by their generative models. We then fit the generative models and their weights until it best fits the observed data. The simplest way to do this is by using the expectation-maximization (EM) algorithm.\n",
    "\n",
    "The EM algorithm finds the maximum likelihood estimators in latent variable models. There are only two steps in EM algorithm as follows:\n",
    "1. **E-Step:** Estimate the missing variables in the dataset.\n",
    "2. **M-Step:** Maximize the parameters of the model in the presence of the data.\n",
    "\n",
    "The most popular method for probability-clustering is by the use of mixture models. We will discuss Gausian mixture model in this notebook.\n",
    "\n",
    "## Gaussian Mixture Model\n",
    "\n",
    "A mixture model consists of an unspecified combination of multiple probability distribution functions. Here, the learning algorithm estimates the parameters of the probability distributions to best fit the density of a given training dataset.\n",
    "\n",
    "So for the Gaussian Mixture Model (GMM), it uses a combination of Gaussian (Normal) probability distributions that requires the estimation of the mean and standard deviation parameters for each. In estimating the parameters, the most common method is the *maximum likelihood estimate*.\n",
    "\n",
    "Consider a dataset whose points happen to be generated by **two different processes**. The points for each process have a Gaussian probability distribution, but the data is combined and the distributions are very similar that it is **not obvious** to which distribution a point may belong.\n",
    "\n",
    "The processes used to generate the data point represents a **latent variable**, e.g. process 0 and process 1. Latent variable influences the data but is not observable. For this case, the EM algorithm is appropriate to estimate the parameters of the distributions.\n",
    "\n",
    "In the EM algorithm, the estimation-step would estimate the latent variable for each data point, and the maximization step would optimize the parameters of the probability distributions that best captures the **density of the data**. The process is repeated until a good set of latent values and a maximum likelihood is achieved that fits the data.\n",
    "\n",
    "1. **E-Step:** Estimate the expected value for each latent variable.\n",
    "2. **M-Step:** Optimize the parameters of the distribution using maximum likelihood.\n",
    "3. Repeat until a good set best fitst the data.\n",
    "\n",
    "Here is a diagram showing the covariance matrix and gaussian formula in one-dimension.\n",
    "<img src = 'figures/cov_gmm.PNG' width= 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-passage",
   "metadata": {},
   "source": [
    "### Mathematics of GMM\n",
    "\n",
    "GMM is a soft clustering algorithm where each cluster corresponds to a generative model that aims to discover the parameters of a probability distribution (e.g., mean, covariance, density function…) for a given cluster (its own probability distribution governs each cluster). \n",
    "\n",
    "$P(y_1,...,y_n|x_1,...,x_n, \\theta) = P(x_1,...,x_n, y_1,...,y_n|\\theta)(joint) / P(x_1,...,x_n|\\theta)$\n",
    "\n",
    "The process of learning is to fit a gaussian model to the data points using maximum likelihood estimation. The Gaussian Mixture model assumes that the clusters are distributed in a normal distribution in n-dimensional space. Here, the purpose is to find a parameter θ that maximized the probability of the observed data.\n",
    "\n",
    "$\\theta_{ML} = \\underset{\\theta}{\\arg\\max} P(x_1,...,x_n|\\theta))$\n",
    "\n",
    "The goal is to compute the conditional distribution of the latent attributes given the observed dataset.\n",
    "$P(x_{n+1}, y_{n+1}|x_1,...,x_n, \\theta)$\n",
    "\n",
    "Finally, the algorithm finds a class that maximizes the probability of the future data given the learned parameters $\\theta$:\n",
    "$\\underset{c}{\\arg\\max} P(x_{n+1}|\\theta_c))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-ceiling",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We start off by demonstrating the advantage of GMM over k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-thriller",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:09:40.785996Z",
     "start_time": "2021-07-31T04:09:39.788153Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-worship",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:09:41.182324Z",
     "start_time": "2021-07-31T04:09:40.788400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y_true = make_blobs(n_samples=400, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "X = X[:, ::-1] # flip axes for better plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-journalism",
   "metadata": {},
   "source": [
    "Weakness of k-means:\n",
    "1. Non-probabilistic nature of k-means and \n",
    "2. Use of simple distance-from-cluster-center to assign cluster membership leads to poor performance for many real-world situations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-wholesale",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:09:41.555193Z",
     "start_time": "2021-07-31T04:09:41.184421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the data with K Means Labels\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(4, random_state=0)\n",
    "labels = kmeans.fit(X).predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-chamber",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T06:26:48.796870Z",
     "start_time": "2021-07-29T06:26:48.793250Z"
    }
   },
   "source": [
    "From the example above, we may not be very confident about the result as there appears to be a  slight overlap between the two middle clusters. One of the motivations of GM model is the fact that $k$-means clustering has no intrinsic measure of probability or uncertainty of cluster assignments.\n",
    "\n",
    "One way to think about the $k$-means model is that it places a circle (in 2D) at the center of each cluster whose radius is set by the most distant point in the cluster and becomes the cut-off for cluster assignment within the training set. Any point outside this circle is not considered a member of the cluster. Let us visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-elite",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:09:41.563075Z",
     "start_time": "2021-07-31T04:09:41.557035Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None):\n",
    "    labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # plot the input data\n",
    "    ax = ax or plt.gca()\n",
    "    ax.axis('equal')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "\n",
    "    # plot the representation of the KMeans model\n",
    "    centers = kmeans.cluster_centers_\n",
    "    print(centers.shape, centers)\n",
    "    radii = [cdist(X[labels == i], [center]).max()\n",
    "             for i, center in enumerate(centers)]\n",
    "    for c, r in zip(centers, radii):\n",
    "        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-platform",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:09:41.827061Z",
     "start_time": "2021-07-31T04:09:41.564760Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "plot_kmeans(kmeans, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-gather",
   "metadata": {},
   "source": [
    "But what if we transform the data? Can k-means still cluster it well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-closer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:09:42.103035Z",
     "start_time": "2021-07-31T04:09:41.828674Z"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(13)\n",
    "X_stretched = np.dot(X, rng.randn(2, 2))\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "plot_kmeans(kmeans, X_stretched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-second",
   "metadata": {},
   "source": [
    "We see that circular k-means cannot cluster the data well at the lower right area and that the circular cut-offs overlap.\n",
    "\n",
    "These two disadvantages of k-means—its lack of flexibility in cluster shape and lack of probabilistic cluster assignment— resulting to poor clustering.\n",
    "\n",
    "To address this issue we can allow: (1) uncertainty in cluster assignment by comparing the distances of each point to all cluster centers and (2) cluster boundaries to be ellipses. These two are the essential components of a different type of Gaussian mixture models.\n",
    "\n",
    "A Gaussian mixture model (GMM) attempts to find a mixture of multi-dimensional Gaussian probability distributions that best model any input dataset. In the simplest case, GMMs can be used for finding clusters in the same manner as k-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-karma",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:09:42.313314Z",
     "start_time": "2021-07-31T04:09:42.105611Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "model = mixture.GaussianMixture(n_components=4, covariance_type='full')\n",
    "\n",
    "gmm = model.fit(X)\n",
    "labels = gmm.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-armor",
   "metadata": {},
   "source": [
    "GMM also allows us to  find probabilistic cluster assignments, which measures the probability that any point belongs to the given cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-australia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:09:42.319857Z",
     "start_time": "2021-07-31T04:09:42.315264Z"
    }
   },
   "outputs": [],
   "source": [
    "probs = gmm.predict_proba(X)\n",
    "print(probs[:5].round(3)) # [n_samples, n_clusters]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-panic",
   "metadata": {},
   "source": [
    "We can visualize this uncertainty (size of each point~certainty of its prediction). The points at the boundaries between clusters reflect the uncertainty of cluster assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-window",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:09:42.652644Z",
     "start_time": "2021-07-31T04:09:42.321450Z"
    }
   },
   "outputs": [],
   "source": [
    "size = 50 * probs.max(1) ** 2  # square emphasizes differences\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=size);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-alexandria",
   "metadata": {},
   "source": [
    "Recall:\n",
    "\n",
    "1. E-step: for each point, find weights encoding the probability of membership in each cluster\n",
    "2. M-step: for each cluster, update its location, normalization, and shape based on all data points, making use of the weights. The result of this step is each cluster associated not with a hard-edged sphere, but with a smooth Gaussian model. Just as in the k-means expectation–maximization approach, this algorithm can sometimes miss the globally optimal solution, and thus in practice multiple random initializations are used.\n",
    "\n",
    "Let's create a function that will help us visualize the locations and shapes of the GMM clusters by drawing ellipses based on the GMM output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-dressing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:33:14.047928Z",
     "start_time": "2021-07-31T04:33:14.033781Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs))\n",
    "        \n",
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-notification",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:09:42.950081Z",
     "start_time": "2021-07-31T04:09:42.664383Z"
    }
   },
   "outputs": [],
   "source": [
    "gmm = model.fit(X)\n",
    "plot_gmm(gmm, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-range",
   "metadata": {},
   "source": [
    "We also demonstrate that GMM is efficient in clustering transformed data sets unlike k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-charlotte",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T04:09:43.238531Z",
     "start_time": "2021-07-31T04:09:42.951851Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_gmm(gmm, X_stretched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c42f4d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1fc6e864617b017beb073c44d027a916",
     "grade": false,
     "grade_id": "cell-45c15d80dbfa47b5",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Exercise 6 [3 pts]**\n",
    "\n",
    "Perform GMM clustering on the `pickup_longitude`, `pickup_latitude`, `dropoff_longitude` and `dropoff_latitude` of `/mnt/data/public/nyctaxi/trip_data/trip_data_4.csv`. Clean data as appropriate. Justify choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e8c03",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "671d2312568ac06ca097a61207ddc9b7",
     "grade": false,
     "grade_id": "cell-b187fe0862b3037e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Exercise 7 [2 pts]**\n",
    "\n",
    "Compare the results of the previous exercises. Based on these, what are the advantages and disadvantages of GMM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a5f782",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c315900",
   "metadata": {},
   "source": [
    "* https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html\n",
    "* https://towardsdatascience.com/17-clustering-algorithms-used-in-data-science-mining-49dbfa5bf69a#a536\n",
    "* https://machinelearningmastery.com/expectation-maximization-em-algorithm/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
