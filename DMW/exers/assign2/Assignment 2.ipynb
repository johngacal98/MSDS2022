{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:48:59.577118Z",
     "start_time": "2021-07-31T17:48:58.934997Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e5c25aed692e3482eac719958909fbf",
     "grade": false,
     "grade_id": "cell-ae6ea22d3ddcf4ce",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import sqlite3\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy.testing import assert_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "916090182d84e1e48344c75f07792a3b",
     "grade": false,
     "grade_id": "cell-5922a14acab6bb5b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For Problems 1 to 4, refer to the SQLite database at `/mnt/data/public/imdb.db`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "680bc59907334fd00568ccc474cf6ef5",
     "grade": false,
     "grade_id": "cell-6f040e1824a95c2e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Problem 1\n",
    "\n",
    "Create a function `count_keywords` that will return a SQL statement and a connection string such that the resulting table has columns `title` and `keywords`, and the rows are `movies` with their number of unique `keywords`. Rows should be sorted by decreasing number of `keywords` then increasing lexicographic order of `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:48:59.582247Z",
     "start_time": "2021-07-31T17:48:59.579020Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5bf66b8718eda924f66ea4af475c467",
     "grade": false,
     "grade_id": "cell-7d403dedbfdb31b5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def count_keywords():\n",
    "    \"\"\"Return a SQL statement and a connection string such that the resulting \n",
    "    table has columns title and keywords, and the rows are movies with their \n",
    "    number of unique keywords\n",
    "    \"\"\"\n",
    "    with sqlite3.connect('/mnt/data/public/imdb.db') as conn:\n",
    "        query = \"\"\"\n",
    "        SELECT m.title, COUNT(DISTINCT(mk.idkeywords)) as keywords\n",
    "        FROM movies as m\n",
    "        JOIN movies_keywords as mk\n",
    "        ON m.idmovies = mk.idmovies\n",
    "        GROUP BY m.idmovies\n",
    "        ORDER BY keywords DESC, m.title ASC\n",
    "        \"\"\"\n",
    "    return (query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:46:01.681231Z",
     "start_time": "2021-07-31T17:46:01.670948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movies',), ('movies_genres',), ('genres',), ('series',), ('movies_keywords',), ('keywords',), ('aka_titles',)]\n"
     ]
    }
   ],
   "source": [
    "# conn = sqlite3.connect('/mnt/data/public/imdb.db')\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# table_list = [a for a in cursor.execute(\"SELECT name FROM sqlite_master WHERE type = 'table'\")]\n",
    "# print(table_list)\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:05.985807Z",
     "start_time": "2021-07-31T17:49:04.821397Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b094ae542ac9fca43219b99e0786f5c",
     "grade": true,
     "grade_id": "cell-71cc3e5237f5aee3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sql, conn = count_keywords()\n",
    "df_keywords = pd.read_sql(sql, conn)\n",
    "assert_equal(df_keywords.shape, (61383, 2))\n",
    "assert_equal(df_keywords.columns.tolist(), ['title', 'keywords'])\n",
    "assert_equal(\n",
    "    df_keywords.iloc[:10].to_numpy().tolist(),\n",
    "    [['The Stand', 435],\n",
    "     ['Rose Red', 333],\n",
    "     ['Girls Bravo', 322],\n",
    "     ['Life', 311],\n",
    "     ['Kenp denki beruseruku', 247],\n",
    "     ['Going Postal', 231],\n",
    "     ['Aika', 229],\n",
    "     ['Anne of Green Gables', 229],\n",
    "     ['Ikki tsen', 226],\n",
    "     ['Empire Falls', 220]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1aa493840575fb7c2383f59cf49b8776",
     "grade": false,
     "grade_id": "cell-acb9dbfb763ecade",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Problem 2\n",
    "\n",
    "Create a function `has_keyword` that will return a SQL statement and a connection string such that the resulting table has column `title` and the rows are unique `title`s (not movies) that have the given case-insensitive keyword. Rows should be sorted by increasing lexicographic order of `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:05.990671Z",
     "start_time": "2021-07-31T17:49:05.987777Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e8269652b0d79d397c8e7a0814dc1a2",
     "grade": false,
     "grade_id": "cell-b6ee05a2990ecf65",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def has_keyword():\n",
    "    \"\"\"Return a SQL statement and a connection string such that the resulting \n",
    "    table has column title and the rows are unique titles (not movies) that \n",
    "    have the given case-insensitive keyword.\n",
    "    \"\"\"\n",
    "    with sqlite3.connect('/mnt/data/public/imdb.db') as conn:\n",
    "        query = \"\"\"\n",
    "            SELECT DISTINCT m.title\n",
    "            FROM movies_keywords as mk\n",
    "            INNER JOIN keywords as k ON mk.idkeywords = k.idkeywords\n",
    "            INNER JOIN movies as m ON mk.idmovies = m.idmovies\n",
    "            WHERE k.keyword LIKE LOWER(?)\n",
    "            ORDER BY m.title ASC\n",
    "    \"\"\"\n",
    "    return (query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:09.310037Z",
     "start_time": "2021-07-31T17:49:05.992450Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f0bd4cf3523b0535f141fee8e9e23e3",
     "grade": true,
     "grade_id": "cell-e41c2d980aecadb0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sql, conn = has_keyword()\n",
    "df_keyword = pd.read_sql(sql, conn, params=['data'])\n",
    "assert_equal(df_keyword.columns.tolist(), ['title'])\n",
    "assert_equal(\n",
    "    df_keyword.to_numpy().tolist(),\n",
    "    [['24'],\n",
    "     ['Cyberchase'],\n",
    "     ['Person of Interest'],\n",
    "     ['Pine Gap'],\n",
    "     ['The Raising of America'],\n",
    "     ['Unnatural Causes']]\n",
    ")\n",
    "df_keyword = pd.read_sql(sql, conn, params=['science'])\n",
    "assert_equal(df_keyword.shape, (424, 1))\n",
    "assert_equal(df_keyword.columns.tolist(), ['title'])\n",
    "assert_equal(\n",
    "    df_keyword.iloc[:10].to_numpy().tolist(),\n",
    "    [['100 Greatest Discoveries'],\n",
    "     ['2025: Webisodes'],\n",
    "     ['2057'],\n",
    "     ['3-2-1 Contact'],\n",
    "     ['5 kulmaa kosmologiaan'],\n",
    "     ['@discovery.ca'],\n",
    "     ['A Window Looking In'],\n",
    "     ['Abenteuer Forschung'],\n",
    "     ['After the Warming'],\n",
    "     ['Against the Elements']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cde0874b0b18861afd7f7e53105b2504",
     "grade": false,
     "grade_id": "cell-950935df9d588fe0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Problem 3\n",
    "\n",
    "Create a function `aka_phils` that will return a SQL statement and a connection string such that the resulting table has columns `year` and `title`, and the rows are the year and the number of movies with an alternate title for the `location` `Philippines: English title`. Return only years that have at least 10 movies. Sort by decreasing `count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:09.314737Z",
     "start_time": "2021-07-31T17:49:09.311785Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61bc31008194f0dbe2c19f0587899a0e",
     "grade": false,
     "grade_id": "cell-dfab62b43d4b2176",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def aka_phils():\n",
    "    \"\"\"Return a SQL statement and a connection string such that the resulting\n",
    "    table has columns year and title, and the rows are the year and the number\n",
    "    of movies with an alternate title for the location Philippines: English\n",
    "    title. Return only years that have at least 10 movies. Sort by decreasing \n",
    "    count.\n",
    "    \"\"\"\n",
    "    with sqlite3.connect('/mnt/data/public/imdb.db') as conn:\n",
    "        query = \"\"\"SELECT a.year, COUNT(a.title) as title\n",
    "            FROM aka_titles AS a\n",
    "            WHERE a.location = 'Philippines: English title'\n",
    "            GROUP BY a.year\n",
    "            HAVING COUNT(a.title) >= 10\n",
    "            ORDER BY title DESC\"\"\"\n",
    "    return (query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:09.341691Z",
     "start_time": "2021-07-31T17:49:09.318194Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71f75ed1350395ff1ab34fa80bb9632c",
     "grade": true,
     "grade_id": "cell-d5ece5ba3076ca4c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sql, conn = aka_phils()\n",
    "df_aka = pd.read_sql(sql, conn)\n",
    "assert_equal(df_aka.shape, (5, 2))\n",
    "assert_equal(\n",
    "    df_aka.iloc[:3].to_numpy().tolist(),\n",
    "    [[2010.0, 30.0],\n",
    "     [2009.0, 27.0],\n",
    "     [2008.0, 23.0]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T02:08:37.824033Z",
     "start_time": "2019-05-27T02:08:37.818390Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c79f7d55a86f1b475d5dcfbf5892e12a",
     "grade": false,
     "grade_id": "cell-4b6650f0b3e6e99f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Problem 4\n",
    "\n",
    "Create a function `convert_twitter` that returns a SQLite connection to an in-memory database. It should contain a table with the following columns:\n",
    " - `id` (integer)\n",
    " - `text` (text)\n",
    " - `is_quote_status` (boolean)\n",
    " - `favorite_count` (integer)\n",
    " - `created_at` (text)\n",
    " - `timestamp_ms` (integer)\n",
    "\n",
    "Each row corresponds to a tweet in `data_twitter_sample.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:09.348652Z",
     "start_time": "2021-07-31T17:49:09.344858Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3dfad6da4a51a4c286b248137af1642",
     "grade": false,
     "grade_id": "cell-bee6c8bdcad14626",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_twitter():\n",
    "    \"\"\"Returns a SQLite connection to an in-memory database.\"\"\"\n",
    "    df = (pd.read_json('data_twitter_sample.json',\n",
    "                      lines=True,\n",
    "                      convert_dates=False)[['id',\n",
    "                                            'text',\n",
    "                                            'is_quote_status',\n",
    "                                            'favorite_count',\n",
    "                                            'created_at',\n",
    "                                            'timestamp_ms']])\n",
    "          \n",
    "    df['timestamp_ms'] = df['timestamp_ms'].astype(int)\n",
    "    \n",
    "    conn = sqlite3.connect(':memory:')\n",
    "    df.to_sql('twitter', conn, index=False)\n",
    "    conn.commit()\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:10.550634Z",
     "start_time": "2021-07-31T17:49:09.350816Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b8547112540184822f24e32305586bc",
     "grade": true,
     "grade_id": "cell-f4c6b7fd43629624",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "conn = convert_twitter()\n",
    "conn.rollback()\n",
    "assert isinstance(conn, sqlite3.Connection)\n",
    "assert_equal(\n",
    "    tuple(conn.execute('SELECT count(*) FROM twitter')),\n",
    "    ((8908,),)\n",
    ")\n",
    "assert_equal(\n",
    "    tuple(conn.execute(\n",
    "        'SELECT * FROM twitter WHERE id = 1014298721671139328')),\n",
    "    ((1014298721671139328, 'get ready for thos shadow bans', 0, 0,\n",
    "      'Wed Jul 04 00:03:54 +0000 2018', 1530662634662),)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6aa4342e0fa3eab21da130ac546e96d3",
     "grade": false,
     "grade_id": "cell-05977ea03fc8dcd6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For Problems 5 to 7, you will be scraping the website at `http://jojie.accesslab.aim.edu:9095/dmw_scraping/ASIA.html` which contains links to the 2016 Philippine senatorial elections results for each Overseas Absentee Voting (OAV) precint in Asia as well as links to the other continents where OAV was conducted. On Jojie, you can access it at `http://192.168.212.2:9095/dmw_scraping/ASIA.html`. Do not escape spaces in the URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a9675bb9fae3289368e248a34a24a96",
     "grade": false,
     "grade_id": "cell-ecddc9c65a338686",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Problem 5\n",
    "\n",
    "Create a function `get_continent_urls` that will return a dictionary with continent name as key and absolute url to that OAV page as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:10.556491Z",
     "start_time": "2021-07-31T17:49:10.552502Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54c0f08ea203aff4f3ef71184bb05eb0",
     "grade": false,
     "grade_id": "cell-d4b8026166089b18",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_continent_urls():\n",
    "    \"\"\"Return a dictionary with continent name as key and absolute url to\n",
    "    that OAV page as value.\n",
    "    \"\"\"\n",
    "    url = 'http://192.168.212.2:9095/dmw_scraping/'\n",
    "    asia = 'http://192.168.212.2:9095/dmw_scraping/ASIA.html'\n",
    "    page = requests.get(asia)\n",
    "    soup = BeautifulSoup(page.text)\n",
    "    dict_ = {}\n",
    "    for i in soup.find_all('h3'):\n",
    "        dict_[i.text] = (url + i.find('a')['href'])\n",
    "    dict_['ASIA'] = asia\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:10.589349Z",
     "start_time": "2021-07-31T17:49:10.559564Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8f307baed9dc1a68eebc81c6d2a3876",
     "grade": true,
     "grade_id": "cell-4d926fc7e759a779",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "continent_url = get_continent_urls()\n",
    "assert_equal(len(continent_url), 4)\n",
    "assert_equal(\n",
    "    continent_url['MIDDLE EAST AND AFRICAS'], \n",
    "    'http://192.168.212.2:9095/dmw_scraping/MIDDLE EAST AND AFRICAS.html'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a392543595119830d9aea7122f86563",
     "grade": false,
     "grade_id": "cell-7e8e0bb8edbbcc6f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Problem 6\n",
    "\n",
    "Create a function `get_fsp_urls` that accepts a `continent` and returns a dictionary with the foreign service posts in that `continent` as key and the list of urls to the precinct JSON files as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:10.597332Z",
     "start_time": "2021-07-31T17:49:10.591088Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b3464e4111373928835ca9fe044a2c0",
     "grade": false,
     "grade_id": "cell-4082b035ed40b7c7",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_fsp_urls(continent):\n",
    "    \"\"\"Accept a continent and returns a dictionary with the foreign service\n",
    "    posts in that continent as key and the list of urls to the precinct JSON\n",
    "    files as values.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    continent: str\n",
    "    \n",
    "    Return:\n",
    "    ------------\n",
    "    dict6: dictionary\n",
    "    \"\"\"\n",
    "    def get_continent_urls():\n",
    "        \"\"\"Return a dictionary with continent name as key and absolute url to\n",
    "        that OAV page as value.\n",
    "        \"\"\"\n",
    "        url = 'http://192.168.212.2:9095/dmw_scraping/'\n",
    "        asia = 'http://192.168.212.2:9095/dmw_scraping/ASIA.html'\n",
    "        page = requests.get(asia)\n",
    "        soup = BeautifulSoup(page.text)\n",
    "        dict_ = {}\n",
    "        for i in soup.find_all('h3'):\n",
    "            dict_[i.text] = (url + i.find('a')['href'])\n",
    "        dict_['ASIA'] = asia\n",
    "        return dict_\n",
    "\n",
    "    base_url = 'http://192.168.212.2:9095/dmw_scraping/'\n",
    "    url = get_continent_urls()[continent]\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text)\n",
    "    dict6 = {}\n",
    "    keys = pd.Series(\n",
    "        [\" \".join(i.text.split()[:-2]) for i in soup.find_all('a')[4:]]\n",
    "    ).unique()\n",
    "    for key in keys:\n",
    "        li = [i['href'] for i in soup.find_all('a') if key in i.text]\n",
    "        dict6[key] = [base_url + i for i in li]\n",
    "    return dict6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:10.642693Z",
     "start_time": "2021-07-31T17:49:10.599236Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "061c2cda9c6ec45c9bff273807539627",
     "grade": true,
     "grade_id": "cell-27760e74e437f517",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "fsp_urls = get_fsp_urls('ASIA')\n",
    "assert_equal(\n",
    "    sorted(fsp_urls.keys()),\n",
    "    ['HONGKONG PCG',\n",
    "     'KUALA LUMPUR PE',\n",
    "     'OSAKA PCG',\n",
    "     'SEOUL PE',\n",
    "     'SINGAPORE PE',\n",
    "     'TOKYO PE']\n",
    ")\n",
    "assert_equal(\n",
    "    sorted(fsp_urls['TOKYO PE']),\n",
    "    ['http://192.168.212.2:9095/dmw_scraping/ASIA/JAPAN/TOKYO PE/90160021/'\n",
    "     'SENATOR PHILIPPINES.json',\n",
    "     'http://192.168.212.2:9095/dmw_scraping/ASIA/JAPAN/TOKYO PE/90160023/'\n",
    "     'SENATOR PHILIPPINES.json',\n",
    "     'http://192.168.212.2:9095/dmw_scraping/ASIA/JAPAN/TOKYO PE/90160024/'\n",
    "     'SENATOR PHILIPPINES.json',\n",
    "     'http://192.168.212.2:9095/dmw_scraping/ASIA/JAPAN/TOKYO PE/90160033/'\n",
    "     'SENATOR PHILIPPINES.json',\n",
    "     'http://192.168.212.2:9095/dmw_scraping/ASIA/JAPAN/TOKYO PE/90160035/'\n",
    "     'SENATOR PHILIPPINES.json',\n",
    "     'http://192.168.212.2:9095/dmw_scraping/ASIA/JAPAN/TOKYO PE/90160038/'\n",
    "     'SENATOR PHILIPPINES.json',\n",
    "     'http://192.168.212.2:9095/dmw_scraping/ASIA/JAPAN/TOKYO PE/90160040/'\n",
    "     'SENATOR PHILIPPINES.json',\n",
    "     'http://192.168.212.2:9095/dmw_scraping/ASIA/JAPAN/TOKYO PE/90160041/'\n",
    "     'SENATOR PHILIPPINES.json']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0170d6fb1c5a3c8a4d6f6faf51a75fb4",
     "grade": false,
     "grade_id": "cell-0a549f7f4bd3aa9c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem 7\n",
    "Create a function `senator_votes` that returns a dictionary with the candidate name as key and the total votes received in the given `continent` and `fsp` as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:10.652498Z",
     "start_time": "2021-07-31T17:49:10.644567Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "767e1c43c4c7830e7d2ecdc513c8cb31",
     "grade": false,
     "grade_id": "cell-fe19def2e98b5178",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def senator_votes(continent, fsp):\n",
    "    \"\"\"Returns a dictionary with the candidate name as key and the total votes\n",
    "    received in the given continent and fsp as values.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    continent: str\n",
    "    fsp: str\n",
    "    \n",
    "    Return:\n",
    "    ------------\n",
    "    int\n",
    "    \"\"\"\n",
    "    def get_fsp_urls(continent):\n",
    "        \"\"\"Accept a continent and returns a dictionary with the foreign service\n",
    "        posts in that continent as key and the list of urls to the precinct JSON\n",
    "        files as values.\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "        continent: str\n",
    "\n",
    "        Return:\n",
    "        ------------\n",
    "        dict6: dictionary\n",
    "        \"\"\"\n",
    "        def get_continent_urls():\n",
    "            \"\"\"Return a dictionary with continent name as key and absolute url to\n",
    "            that OAV page as value.\n",
    "            \"\"\"\n",
    "            url = 'http://192.168.212.2:9095/dmw_scraping/'\n",
    "            asia = 'http://192.168.212.2:9095/dmw_scraping/ASIA.html'\n",
    "            page = requests.get(asia)\n",
    "            soup = BeautifulSoup(page.text)\n",
    "            dict_ = {}\n",
    "            for i in soup.find_all('h3'):\n",
    "                dict_[i.text] = (url + i.find('a')['href'])\n",
    "            dict_['ASIA'] = asia\n",
    "            return dict_\n",
    "\n",
    "        base_url = 'http://192.168.212.2:9095/dmw_scraping/'\n",
    "        url = get_continent_urls()[continent]\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text)\n",
    "        dict6 = {}\n",
    "        keys = pd.Series(\n",
    "            [\" \".join(i.text.split()[:-2]) for i in soup.find_all('a')[4:]]\n",
    "        ).unique()\n",
    "        for key in keys:\n",
    "            li = [i['href'] for i in soup.find_all('a') if key in i.text]\n",
    "            dict6[key] = [base_url + i for i in li]\n",
    "        return dict6\n",
    "\n",
    "    df7 = pd.DataFrame()\n",
    "\n",
    "    for i in get_fsp_urls(continent)[fsp]:\n",
    "\n",
    "\n",
    "        df_sen = pd.DataFrame.from_dict(\n",
    "            json.loads(requests.get(i).text)['results']\n",
    "        )\n",
    "\n",
    "\n",
    "        df7 = df7.append(df_sen)\n",
    "\n",
    "    df7['votes'] = df7['votes'].astype(int)\n",
    "    return dict(df7.groupby('bName')['votes'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:10.746128Z",
     "start_time": "2021-07-31T17:49:10.654242Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a582ad5f44fb46d64cfa37bc041e9405",
     "grade": true,
     "grade_id": "cell-8021aa6ce74c25d9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "can_votes = senator_votes('ASIA', 'TOKYO PE')\n",
    "assert_equal(len(can_votes), 50)\n",
    "assert_equal(can_votes['PACQUIAO, MANNY (UNA)'], 1595)\n",
    "assert_equal(can_votes['SOTTO, VICENTE (NPC)'], 1545)\n",
    "assert_equal(can_votes['GADON, LARRY (KBL)'], 105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "783d100ad6baa570e58f824997d47c93",
     "grade": false,
     "grade_id": "cell-451b06aeb8c24e01",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Problem 8\n",
    "Create a function `get_books` that returns a list of tuples with book title as the first element and paperback selling price as the second element for all the books in `data_wrangling.html`. Set price to `None` if not there's no price indicated. Sort them in order of appearance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:10.752931Z",
     "start_time": "2021-07-31T17:49:10.747979Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df3b66557012479d40f0d05290ef1d59",
     "grade": false,
     "grade_id": "cell-a0251c34af0a81d2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_books():\n",
    "    \"\"\"Return a list of tuples with book title as the first element and\n",
    "    paperback selling price as the second element for all the books in\n",
    "    data_wrangling.html. Set price to None if not there's no price indicated.\n",
    "    Sort them in order of appearance.\n",
    "    \"\"\"\n",
    "    with open('data_wrangling.html') as f:\n",
    "        soup = BeautifulSoup(f)\n",
    "    div = soup.find_all('div', class_='sg-col-4-of-12 sg-col-8-of-16 sg-col-'\n",
    "                        '16-of-24 sg-col-12-of-20 sg-col-24-of-32 sg-col '\n",
    "                        'sg-col-28-of-36 sg-col-20-of-28')\n",
    "\n",
    "    li = []\n",
    "\n",
    "    for elem in div:\n",
    "        title = (elem.find('span', \n",
    "                           class_='a-size-medium a-color-base a-text-normal')\n",
    "                 .text)\n",
    "\n",
    "        try:\n",
    "            price = (elem.find('span', class_='a-size-base a-color-base')\n",
    "                     .findNext('span').text)\n",
    "        except AttributeError:\n",
    "            whole = elem.find('span', class_='a-offscreen')\n",
    "\n",
    "            try:\n",
    "                price = whole.text\n",
    "            except AttributeError:\n",
    "                price = None\n",
    "\n",
    "        li.append((title, price))\n",
    "    return li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:10.921218Z",
     "start_time": "2021-07-31T17:49:10.754771Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d6cccb1c21c9802c79d33420756ce85",
     "grade": true,
     "grade_id": "cell-8a049433fd5f7094",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "title_price = get_books()\n",
    "assert_equal(len(title_price), 16)\n",
    "assert_equal(\n",
    "    title_price[:10],\n",
    "    [('Python for Data Analysis: Data Wrangling with Pandas, NumPy, and '\n",
    "      'IPython', '$35.65'),\n",
    "     ('Data Wrangling with Python: Tips and Tools to Make Your Life Easier',\n",
    "      '$32.59'),\n",
    "     ('Data Wrangling with R (Use R!)', '$67.49'),\n",
    "     ('Data Wrangling with Python: Creating actionable data from raw sources',\n",
    "      '$39.99'),\n",
    "     ('Practical Data Wrangling: Expert techniques for transforming your raw '\n",
    "      'data into a valuable source for analytics', '$29.99'),\n",
    "     ('Python Data Science Handbook: Essential Tools for Working with Data',\n",
    "      '$28.12'),\n",
    "     ('Think Like a Data Scientist: Tackle the data science process '\n",
    "      'step-by-step', '$27.92'),\n",
    "     ('Principles of Data Wrangling: Practical Techniques for Data '\n",
    "      'Preparation', '$27.83'),\n",
    "     ('Data Wrangling with JavaScript', '$40.78'),\n",
    "     ('Data Wrangling: Munging in R with SQL and MongoDB for Financial '\n",
    "      'Applications', '$45.39')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3049871433edb646800e87fc0a63e24",
     "grade": false,
     "grade_id": "cell-f8a3f6c885ed19a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem 9\n",
    "\n",
    "Create a function `get_revisions_timeseries` that returns a `pandas` `Series` with index equal to months in `YYYY-MM` format and values corresponding to the number of revisions made in the English Wikipedia article `Data science` for that month. Include months since creation of the article until before July 2021 but exclude those months where no revision was made. Sort by chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:10.929186Z",
     "start_time": "2021-07-31T17:49:10.923013Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75aadcbfcff166dcc604cad7b2d1730c",
     "grade": false,
     "grade_id": "cell-3f83809ad3de175d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_revisions_timeseries():\n",
    "    \"\"\"Return a pandas Series with index equal to months in YYYY-MM format and\n",
    "    values corresponding to the number of revisions made in the English\n",
    "    Wikipedia article Data science for that month. Include months since\n",
    "    creation of the article until before July 2021 but exclude those months\n",
    "    where no revision was made. Sort by chronological order.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'revisions',\n",
    "        'titles': 'Data science',\n",
    "        'format': 'json',\n",
    "        'rvlimit': 'max',\n",
    "        'rvprop': 'ids|timestamp',\n",
    "        'continue': '||',\n",
    "        'rvend': '2021-07-01T00:00:00',\n",
    "        'rvdir': 'newer'                   \n",
    "    }\n",
    "\n",
    "    df9 = pd.DataFrame()\n",
    "    looper = True\n",
    "    while looper:\n",
    "        res = requests.get('https://en.wikipedia.org/w/api.php',\n",
    "                          params=params)\n",
    "        data = res.json()\n",
    "        display(data.keys())\n",
    "        query = data['query']\n",
    "        page_index = list(data['query']['pages'].keys())[0]\n",
    "        x = query['pages'][page_index]['revisions']\n",
    "        df = pd.DataFrame(x)\n",
    "        df9 = df9.append(df)\n",
    "        if 'continue' in data.keys():\n",
    "            print('continue in data keys')\n",
    "            params['rvcontinue'] = data['continue']['rvcontinue']\n",
    "            print(params['rvcontinue'])\n",
    "        else:\n",
    "            looper = False        \n",
    "\n",
    "    def time(x):\n",
    "        year = str(x.year)\n",
    "        month = str(x.month)\n",
    "\n",
    "        if len(month) != 2:\n",
    "            month = '0'+month\n",
    "        return year + '-' + month\n",
    "\n",
    "    df9['timestamp'] = pd.to_datetime(df9['timestamp'])\n",
    "    df9['timestamp'] = df9['timestamp'].apply(time)\n",
    "    return df9.groupby('timestamp')['revid'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:13.643159Z",
     "start_time": "2021-07-31T17:49:10.930985Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61e6cc4c75a73f4cfa8d101de51e19fa",
     "grade": true,
     "grade_id": "cell-3858eef36ec45ac4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['continue', 'query', 'limits'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue in data keys\n",
      "20160725205502|731513124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['continue', 'query', 'limits'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue in data keys\n",
      "20200130053259|938280158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['batchcomplete', 'query', 'limits'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rev_ts = get_revisions_timeseries()\n",
    "assert_equal(rev_ts.shape, (109,))\n",
    "assert_equal(\n",
    "    rev_ts.index[:10].tolist(),\n",
    "    ['2012-04',\n",
    "     '2012-05',\n",
    "     '2012-06',\n",
    "     '2012-07',\n",
    "     '2012-08',\n",
    "     '2012-10',\n",
    "     '2012-11',\n",
    "     '2012-12',\n",
    "     '2013-01',\n",
    "     '2013-02']\n",
    ")\n",
    "assert_equal(\n",
    "    rev_ts[:10].tolist(),\n",
    "    [12, 6, 13, 10, 9, 16, 16, 9, 28, 11]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2eaf84a7495c179fb44b0cc8b9fb1b41",
     "grade": false,
     "grade_id": "cell-c511a97f4e9981d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem 10\n",
    "\n",
    "Create a function `get_datasci_link_revs_asof` that returns the list of the revision ID, as of 1 July 2021 UTC, of each linked page in revision id `1027747892` of the English Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:13.654051Z",
     "start_time": "2021-07-31T17:49:13.645348Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3e10ce0cf683602d1da37050d771c47",
     "grade": false,
     "grade_id": "cell-001df00ebeac2f4c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_datasci_link_revs_asof():\n",
    "    \"\"\"Return the list of the revision ID, as of 1 July 2021 UTC, of each\n",
    "    linked page in revision id 1027747892 of the English Wikipedia.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'revisions',\n",
    "        'revids': '1027747892',\n",
    "        'format': 'json',\n",
    "        'generator': 'links',\n",
    "        'gpllimit': 'max'\n",
    "    }\n",
    "\n",
    "    res = requests.get('https://en.wikipedia.org/w/api.php',\n",
    "                      params = params)\n",
    "    data = res.json()\n",
    "    df_data = pd.DataFrame()\n",
    "    for i in data['query']['pages']:\n",
    "        df = pd.json_normalize(data['query']['pages'][i],\n",
    "                               'revisions',\n",
    "                               ['pageid', 'ns', 'title'])\n",
    "        df_data = df_data.append(df)\n",
    "    df_data = df_data.reset_index(drop=True)\n",
    "    df_data['timestamp'] = pd.to_datetime(df_data.timestamp)\n",
    "    df_data = df_data[df_data['ns']==0]\n",
    "    df_data = df_data[['title', 'revid', 'timestamp', 'ns']]\n",
    "    df_correct = pd.DataFrame()\n",
    "    for title in df_data.title:\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'prop': 'revisions',\n",
    "            'rvprop': 'ids|timestamp',\n",
    "            'titles': f'{title}',\n",
    "            'format': 'json',\n",
    "            'rvstart': '2021-07-01T00:00:00',\n",
    "            'rvlimit': '1'\n",
    "        }\n",
    "\n",
    "        res_wrong = requests.get('https://en.wikipedia.org/w/api.php',\n",
    "                                 params = params)\n",
    "        data_wrong = res_wrong.json()\n",
    "        page_id = list(data_wrong['query']['pages'].keys())[0]\n",
    "        df = pd.DataFrame(data_wrong['query']['pages'][page_id]['revisions'], index=[title])\n",
    "        df.index.name = 'title'\n",
    "        df = df.reset_index().drop(columns=['parentid'], axis=1)\n",
    "\n",
    "        df_correct = df_correct.append(df)\n",
    "    ser = df_correct.reset_index(drop=True).sort_values(by='timestamp').revid\n",
    "    return ser.tolist() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T17:49:35.770309Z",
     "start_time": "2021-07-31T17:49:13.656091Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a85aab944adc12bda5cb8c53f901c86",
     "grade": true,
     "grade_id": "cell-e4935e606f995412",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "revs_asof = get_datasci_link_revs_asof()\n",
    "assert_equal(len(revs_asof), 50)\n",
    "assert_equal(\n",
    "    revs_asof[:10],\n",
    "    [285436293,\n",
    "     433201973,\n",
    "     650889829,\n",
    "     775426682,\n",
    "     778233465,\n",
    "     889336491,\n",
    "     932609783,\n",
    "     937074297,\n",
    "     946577178,\n",
    "     960265088])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
