{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9a4568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T10:09:47.530101Z",
     "start_time": "2022-02-04T10:09:47.258859Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from numpy.testing import assert_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b06b6673",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T10:09:54.881371Z",
     "start_time": "2022-02-04T10:09:47.532714Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/04 18:09:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/04 18:09:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/02/04 18:09:52 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/02/04 18:09:52 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "            .builder\n",
    "            .master('local[*]')\n",
    "            .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0566d63a",
   "metadata": {},
   "source": [
    "## Problem 2a [10 pts]\n",
    "\n",
    "Create a function `most_submits` that accepts a globstring to [Github archive](https://www.gharchive.org) data then returns a Spark DataFrame with columns `type` and `actor`. The `type` column corresponds to an event and the values correspond to the `actor` `login` who initiated the most number of times of that event type. Sort by `type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cd6b57a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T10:09:54.899551Z",
     "start_time": "2022-02-04T10:09:54.885291Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7f99fd927d54213ff3ef2f7fc3fd610",
     "grade": false,
     "grade_id": "cell-42e7c02590871138",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def most_submits(paths):\n",
    "    from operator import add\n",
    "    from pyspark.sql import functions as f\n",
    "    return (spark.read.json(paths)\n",
    "     .select('type','actor')\n",
    "     .rdd\n",
    "     .map(lambda x: ((x['type'], x['actor']['login']), 1))\n",
    "     .reduceByKey(add)\n",
    "     .map(lambda x: (x[0][0], x[0][1],x[1]))\n",
    "     .toDF(['type', 'login', 'count'])\n",
    "     .orderBy('count', ascending=False)\n",
    "     .drop_duplicates(['type'])\n",
    "     .drop('count')\n",
    "     .orderBy('type')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3cd909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b675ae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T10:12:29.165731Z",
     "start_time": "2022-02-04T10:09:54.904047Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "747722810b713396e1ed017b4d29dadd",
     "grade": true,
     "grade_id": "cell-9701c170c2caf09f",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/04 18:11:11 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pdf_ms = (most_submits('/mnt/localdata/public/gharchive/2020-01-01-*.json.gz')\n",
    "          .toPandas())\n",
    "assert_equal(pdf_ms.columns.tolist(), ['type', 'login'])\n",
    "assert_equal(\n",
    "    pdf_ms.iloc[:5].to_numpy().tolist(),\n",
    "    [['CommitCommentEvent', 'now[bot]'],\n",
    "     ['CreateEvent', 'svc-software-factory'],\n",
    "     ['DeleteEvent', 'dependabot-preview[bot]'],\n",
    "     ['ForkEvent', 'fagan2888'],\n",
    "     ['GollumEvent', 'joric']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f66a875",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a5a289e89ba0711781889aef19bb1b2",
     "grade": false,
     "grade_id": "cell-f4a3dd1f0f5d893a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 2b [15 pts]\n",
    "\n",
    "In this problem, you will be working with Wikipedia clickstream dataset files. It has the following [format](https://meta.wikimedia.org/wiki/Research:Wikipedia_clickstream#Format):\n",
    "\n",
    "* `prev`: the result of mapping the referer URL to the fixed set of values described below.\n",
    "* `curr`: the title of the article the client requested\n",
    "* `type`: describes (`prev`, `curr`)\n",
    "  * `link`: if the referer and request are both articles and the referer links to the request\n",
    "  * `external`: if the referer host is not `en(.m)?.wikipedia.org`\n",
    "  * `other`: if the referer and request are both articles but the referer does not link to the request. This can happen when clients search or spoof their refer.\n",
    "* `n`: the number of occurrences of the (referer, resource) pair\n",
    "  \n",
    "The `prev` column has the following values:\n",
    "\n",
    "* an article in the main namespace -> the article title\n",
    "* a page from any other Wikimedia project -> `other-internal`\n",
    "* an external search engine -> `other-search`\n",
    "* any other external site -> `other-external`\n",
    "* an empty referer -> `other-empty`\n",
    "* anything else -> `other-other`\n",
    "\n",
    "Create a function `count_inbound_articles` that accepts a globstring to Wikipedia clickstream dataset files and returns a Spark dataframe with columns `cur` corresponding to an article and `inbound` corresponding to the number of unique case-sensitive articles from which the article was visited. Sort by decreasing `inbound` and return only the 10 rows with the largest `inbound`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe98e45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T10:12:29.178859Z",
     "start_time": "2022-02-04T10:12:29.169421Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd3dd72b9f3d95ca71d2c46fb57707cc",
     "grade": false,
     "grade_id": "cell-bfb357080b47b044",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def count_inbound_articles(path):\n",
    "    from pyspark.sql.functions import col, countDistinct\n",
    "    return (spark.read.csv(path, sep='\\t', ).toDF('prev', 'cur', 'type', 'n')\n",
    " .filter(col('prev').rlike(r'^(?!other-.*)'))\n",
    " .groupby(['cur'])\n",
    " .agg(countDistinct('prev').alias('inbound'))\n",
    " .orderBy('inbound', ascending=False)\n",
    " .limit(10)\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7156c7a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T10:12:29.804903Z",
     "start_time": "2022-02-04T10:12:29.182015Z"
    }
   },
   "outputs": [],
   "source": [
    "# path = '/mnt/localdata/public/wikipedia/clickstream/clickstream/2019-10/clickstream-enwiki-2019-10.tsv.gz'\n",
    "# wiki = spark.read.csv(path, sep='\\t', ).toDF('prev', 'curr', 'type', 'n')\n",
    "# from pyspark.sql.functions import col, countDistinct\n",
    "# (spark.read.csv(path, sep='\\t', ).toDF('prev', 'curr', 'type', 'n')\n",
    "#  .filter(col('prev').rlike(r'^(?!other-.*)'))\n",
    "#  .groupby(['curr'])\n",
    "#  .agg(countDistinct('prev'))\n",
    "#  .orderBy('count(prev)', ascending=False)\n",
    "#  .limit(10)\n",
    "#  .toPandas()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0af447e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T10:12:31.054190Z",
     "start_time": "2022-02-04T10:12:29.808906Z"
    }
   },
   "outputs": [],
   "source": [
    "# wiki.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a6b28f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T09:44:41.668360Z",
     "start_time": "2022-02-04T09:42:05.159857Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d1404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5be8c96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T10:15:10.551928Z",
     "start_time": "2022-02-04T10:12:31.057567Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dca6b66106a9e944b8f666eb72974ae8",
     "grade": true,
     "grade_id": "cell-59aca83d3533f1d2",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_cia = count_inbound_articles(\n",
    "    '/mnt/localdata/public/wikipedia/clickstream/clickstream/2019-10/'\n",
    "    'clickstream-enwiki-2019-10.tsv.gz'\n",
    ").toPandas()\n",
    "\n",
    "assert_equal(df_cia.shape, (10, 2))\n",
    "assert_equal(\n",
    "    df_cia.columns.tolist(),\n",
    "    ['cur', 'inbound']\n",
    ")\n",
    "assert_equal(\n",
    "    df_cia.iloc[:5].to_numpy().tolist(),\n",
    "    [['Hyphen-minus', 135466],\n",
    "     ['Main_Page', 127434],\n",
    "     ['United_States', 7168],\n",
    "     ['India', 4002],\n",
    "     ['United_Kingdom', 3619]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd4ce61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
