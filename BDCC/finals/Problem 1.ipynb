{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T06:44:25.572104Z",
     "start_time": "2022-03-07T06:44:25.253680Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from numpy.testing import assert_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T06:44:30.044227Z",
     "start_time": "2022-03-07T06:44:25.574560Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/07 14:44:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/07 14:44:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/03/07 14:44:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/03/07 14:44:28 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/03/07 14:44:28 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"local[*]\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "The text file `/mnt/localdata/public/reddit/comments/RC_2019-02-01-1M` contains the first 1 million comments posted on 2019-02-01 on Reddit. Each line is a JSON object corresponding to a comment.\n",
    "\n",
    "For this problem, you may **only** use Apache Spark and the Python Standard Library. You **cannot** use numpy, scipy, pandas or scikit-learn. Do **not** print or display large amount of results. You will get deductions if you make the browser unresponsive. Each assertion cell should finish running within 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T06:44:40.877288Z",
     "start_time": "2022-03-07T06:44:30.046349Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/07 14:44:40 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    " .read.json('/mnt/localdata/public/reddit/comments/RC_2019-02-01-1M')\n",
    " .createOrReplaceTempView('comments')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce04127d3a086c2630bbe271edb2f966",
     "grade": false,
     "grade_id": "cell-bf48899bffc58ac4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 1a [15 pts]\n",
    "\n",
    "Create a function `count_flair_emoji` that returns a SQL statement which will process `comments` to return a Spark DataFrame with two columns: `author` corresponding to a unique `author` and `emoji_flair_count` corresponding to the number of `author_flair_richtext` elements of that `author` that are of `emoji` type. Consider only authors that have more than 2 `author_flair_richtext` elements. Sort by lexicographical order of `author`. \n",
    "\n",
    "Hint: `count_flair_emoji` need not only return a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T06:44:40.959169Z",
     "start_time": "2022-03-07T06:44:40.880717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.emoji_count(x)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf\n",
    "def emoji_count(x):\n",
    "    \n",
    "    return len([elem['e'] for elem in x if elem['e']=='emoji'])\n",
    "\n",
    "spark.udf.register('emoji_count', emoji_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T06:44:40.964348Z",
     "start_time": "2022-03-07T06:44:40.960802Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f866c7480507849a73e77492c9fcae85",
     "grade": false,
     "grade_id": "cell-27d32b6135404d52",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def count_flair_emoji():\n",
    "    statement = \"\"\"\n",
    "    \n",
    "  \n",
    "  \n",
    "    (SELECT DISTINCT(author), emoji_count(author_flair_richtext) AS `emoji_flair_count`\n",
    "    FROM comments \n",
    "    WHERE author IS NOT NULL\n",
    "        AND author_flair_richtext IS NOT NULL \n",
    "        AND SIZE(author_flair_richtext) != 0\n",
    "        AND SIZE(author_flair_richtext) > 2\n",
    "    \n",
    "    ORDER BY author\n",
    "    )\n",
    "   \n",
    "    \n",
    "    \"\"\"\n",
    "    return statement\n",
    "\n",
    "# WHERE author == '-Jack_MeHoff-' or author == '-NoFaithInFate-' or author == '-weirdo-'\n",
    "# author_flair_richtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T06:44:49.433738Z",
     "start_time": "2022-03-07T06:44:40.966044Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be1528d21a240f4319c9836e0eeb10a4",
     "grade": true,
     "grade_id": "cell-6d5cd4c5e02bed82",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_emoji = spark.sql(count_flair_emoji()).limit(10).toPandas()\n",
    "assert_equal(\n",
    "    df_emoji.iloc[:5].to_numpy().tolist(),\n",
    "    [['-Jack_MeHoff-', '3'],\n",
    "     ['-NoFaithInFate-', '3'],\n",
    "     ['-Reddit_Account-', '3'],\n",
    "     ['-badger--', '2'],\n",
    "     ['-weirdo-', '7']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce06bae14cb57e829a290dafa3f4a892",
     "grade": false,
     "grade_id": "cell-7060cd532628c114",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 1b [10 pts]\n",
    "\n",
    "Create a function `count_comments` that returns a SQL statement which will process `comments` to return a Spark DataFrame with three columns: `author` corresponding to a unique `author`, `comment_count` corresponding to the number of comments by the `author` and `subreddit_count` corresponding to the number of subreddits the `author` posted a comment on. Exclude `[deleted]` authors. Sort by decreasing `comment_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T06:44:49.438744Z",
     "start_time": "2022-03-07T06:44:49.435552Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b054c2386c21708b6ebebdf77b1bda5",
     "grade": false,
     "grade_id": "cell-8940a0dd392a108a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def count_comments():\n",
    "    statement = \"\"\"\n",
    "    SELECT *\n",
    "    FROM\n",
    "    (SELECT author, count(*) as `comment_count`, COUNT(DISTINCT(subreddit)) as `subreddit_count`\n",
    "    FROM comments\n",
    "    \n",
    "    GROUP BY author)\n",
    "    \n",
    "    WHERE author != '[deleted]'\n",
    "    ORDER BY comment_count DESC\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    return spark.sql(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T06:45:02.326032Z",
     "start_time": "2022-03-07T06:44:49.440252Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9d209b21a372f09928b5cbe6827b338",
     "grade": true,
     "grade_id": "cell-f622ade592abc179",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_count = count_comments().limit(10).toPandas()\n",
    "assert_equal(\n",
    "    df_count.iloc[:10].to_numpy().tolist(),\n",
    "    [['AutoModerator', 12307, 1461],\n",
    "     ['transcribot', 621, 1],\n",
    "     ['MemeInvestor_bot', 593, 1],\n",
    "     ['KeepingDankMemesDank', 478, 1],\n",
    "     ['transcribersofreddit', 422, 1],\n",
    "     ['Marketron-I', 329, 1],\n",
    "     ['RickyontheRadiator', 318, 3],\n",
    "     ['NFCAAOfficialRefBot', 282, 1],\n",
    "     ['agree-with-you', 279, 240],\n",
    "     ['imguralbumbot', 223, 186]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
