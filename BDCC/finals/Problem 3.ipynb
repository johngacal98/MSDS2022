{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "\n",
    "You will perform the tasks for this problem in a Spark EMR cluster that you will launch. It can be of any size. You may **only** use Apache Spark and the Python Standard Library. You **cannot** use numpy, scipy, pandas or scikit-learn. Do **not** print or display large amount of results. You will get deductions if you make the browser unresponsive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e7b7cfbd2f894064bbc3bceb3b57ba3",
     "grade": false,
     "grade_id": "cell-e98fb2ba72cbd3c9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Problem 3a [5 pts]\n",
    "\n",
    "Place a notebook named `bdcc-fe-cc.ipynb` on a bucket named `bdcc-<username>-2022`. Write all your answers for this part of the exam on that notebook and make sure that the bucket is accessible to the public."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cb1fdf3b31d0320b80488af36e52608",
     "grade": false,
     "grade_id": "cell-c8302b03011a6d1c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Problem 3b [10 pts]\n",
    "\n",
    "Show the total number of crawled URLs in the [Amazon Common Crawl](https://registry.opendata.aws/commoncrawl/) public dataset `commoncrawl/crawl-001/2009/01/01/0/` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text('s3a://commoncrawl/crawl-001/2009/01/01/0/*.gz')\n",
    "df.filter(f.col('value').startswith('x_commoncrawl_OriginalURL:http')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "feb1e08f26c26ac105636d1be62534ab",
     "grade": false,
     "grade_id": "cell-395a675b1a3ce578",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Problem 3c [10 pts]\n",
    "\n",
    "Show a plot of the # of crawled URLs per hour. Use the timestamps stored in `x_commoncrawl_FetchTimestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timestamp = (df\n",
    " .filter(f.col('value').startswith('x_commoncrawl_FetchTimestamp'))\n",
    " .withColumn('unix', (f.substring('value', 30, 13).cast('double') / 1000))\n",
    " .withColumn('timestamp', f.from_unixtime(f.col('unix')))\n",
    " .withColumn('hour', f.hour(f.col('timestamp'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "(df_timestamp\n",
    " .select('hour')\n",
    " .groupBy('hour')\n",
    " .count()\n",
    " .orderBy('hour')\n",
    "#  .limit(5)\n",
    " .toPandas().plot(kind='bar', legend=None))\n",
    "# plt.legend(False)\n",
    "plt.ylabel('Count')\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6ad6b71ef4a19a66f0b3e1390de3b8f",
     "grade": false,
     "grade_id": "cell-295a675b1a3ce578",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Notes:\n",
    "1. Do not delete outputs for your final answer.\n",
    "1. Submit the notebook using Jojie for partial points in case your S3 bucket is not accessible\n",
    "1. You can screenshot the plot, and attach it to the notebook, if the output is not being saved together with the notebook"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
