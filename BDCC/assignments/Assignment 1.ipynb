{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each solution should finish running within 5 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:57:47.115904Z",
     "start_time": "2022-02-03T14:57:46.766721Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.testing import assert_equal, assert_array_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:57:47.222421Z",
     "start_time": "2022-02-03T14:57:47.119712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import mlxtend\n",
    "    print('existing')\n",
    "except:\n",
    "    !pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:57:47.583728Z",
     "start_time": "2022-02-03T14:57:47.226283Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "import fim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:57:55.922164Z",
     "start_time": "2022-02-03T14:57:47.587175Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pos = pd.read_excel('pos_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, you will work with the POS data (`pos_data.xlsx`) of a real restaurant. **Do not** redistribute this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "850fa99467dfb9563ffc508d499b4e33",
     "grade": false,
     "grade_id": "cell-b75b304cd6165076",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Create a function `most_frequent` that returns the 10 most frequent maximal itemsets of the dataset wherein each item is identified by the item ID. Return itemsets by decreasing support then by increasing smallest item ID in the itemset. Sort items in an itemset in increasing order of their numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:57:55.930528Z",
     "start_time": "2022-02-03T14:57:55.924439Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bfde14443c9491b1a6175f7e010765e",
     "grade": false,
     "grade_id": "cell-3bfb215b2004b4b9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def most_frequent():\n",
    "    \n",
    "    from mlxtend.frequent_patterns import fpmax\n",
    "    transactions = [\n",
    "        x for x in df_pos.groupby('BILL_ID')['ITEM_ID']\n",
    "        .apply(lambda x: list(x))\n",
    "    ]\n",
    "    TE = TransactionEncoder()\n",
    "    TE_fit = TE.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(TE_fit, columns = TE.columns_)\n",
    "    df_out = (fpmax(df, min_support=0.01, use_colnames=True)\n",
    "              .sort_values(by='support', ascending=False)\n",
    "             )\n",
    "    df_out['itemsets'] = df_out['itemsets'].apply(lambda x: tuple(sorted(x)))\n",
    "    return list(df_out['itemsets'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:57:56.290242Z",
     "start_time": "2022-02-03T14:57:55.932603Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cd10f97f1db1d6b01953455c4c06eb2",
     "grade": true,
     "grade_id": "cell-6daed6e45d0ef1a7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "max_itemsets = most_frequent()\n",
    "assert_equal(max_itemsets[:5], \n",
    "                   [(587,), (369, 383), (386,), (172,), (378, 383)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c0cc9ea9eaf73a533a76ba4b60b2e19",
     "grade": false,
     "grade_id": "cell-314412bfe73c36e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Create a function `most_lift` that returns the 10 association rules with the most lift. Each rule should have a minimum relative support of 0.01 and minimum confidence of 0.6. List each rule as a tuple of antecedent and consequent. Items in the antecedent should be sorted by increasing numerical value. Sort the rules by decreasing lift, increasing consequent and increasing smallest item ID in the antecedent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:57:56.297234Z",
     "start_time": "2022-02-03T14:57:56.292561Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b3dcb382ed4bac78c9bb7da1198711b",
     "grade": false,
     "grade_id": "cell-23e1dc2b90799240",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def most_lift():\n",
    "    transactions = df_pos.groupby('BILL_ID')['ITEM_ID'].apply(set)\n",
    "    rules = fim.arules(transactions,\n",
    "                       supp = 1,\n",
    "                       conf = 60,\n",
    "                       report = 'l',\n",
    "                       eval = 'l' )\n",
    "    return [(tuple(sorted(val[1])), val[0]) for val in \n",
    "            sorted(rules, key = lambda x: (x[2],x[0]),reverse = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:57:56.437512Z",
     "start_time": "2022-02-03T14:57:56.299114Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7517fff22af841370d06cfbd810b698b",
     "grade": true,
     "grade_id": "cell-d44ecd3b25dd3456",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "rules = most_lift()\n",
    "\n",
    "assert_equal(\n",
    "    rules[:5],\n",
    "    [((654, 657), 651),\n",
    "     ((654, 656), 651),\n",
    "     ((651, 657), 654),\n",
    "     ((646, 651), 654),\n",
    "     ((651, 656), 654)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1c [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99a2162c089b251ce49cb8f11f4370c3",
     "grade": false,
     "grade_id": "cell-7480abecc83148e7",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Provide three recommendations to the owner of the restaurant based on the frequent itemset mining and association rule mining performed on the POS dataset. Each recommendation should be supported by relevant FIM or ARM results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation 1\n",
    "\n",
    "Given the top maximal itemsets, it is recommended that most frequently bought items such as mineral water, potato fries, brewed coffee, Coke 1.5 Liter should always have enough stocks in their inventory in order to maximize sales and to minimize opportunity costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:57:57.041932Z",
     "start_time": "2022-02-03T14:57:56.440112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.045486</td>\n",
       "      <td>[MINERAL WATER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.037557</td>\n",
       "      <td>[SINIGANG NA ISDA, STEAMED RICE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.031196</td>\n",
       "      <td>[POTATO FRIES]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.029540</td>\n",
       "      <td>[BREWED COFFEE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.029366</td>\n",
       "      <td>[NATIVE CHICKEN TINOLA, STEAMED RICE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.028930</td>\n",
       "      <td>[STEAMED RICE, CARAFFE LEMON CUCUMBER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.028581</td>\n",
       "      <td>[TILAPIA 150 PER KILO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.028494</td>\n",
       "      <td>[HAWAIIAN SUPREME]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.027013</td>\n",
       "      <td>[COKE 1.5 LITER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.026926</td>\n",
       "      <td>[BEEF BULALO, STEAMED RICE]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     support                                itemsets\n",
       "84  0.045486                         [MINERAL WATER]\n",
       "80  0.037557        [SINIGANG NA ISDA, STEAMED RICE]\n",
       "68  0.031196                          [POTATO FRIES]\n",
       "65  0.029540                         [BREWED COFFEE]\n",
       "76  0.029366   [NATIVE CHICKEN TINOLA, STEAMED RICE]\n",
       "85  0.028930  [STEAMED RICE, CARAFFE LEMON CUCUMBER]\n",
       "64  0.028581                  [TILAPIA 150 PER KILO]\n",
       "63  0.028494                      [HAWAIIAN SUPREME]\n",
       "61  0.027013                        [COKE 1.5 LITER]\n",
       "69  0.026926             [BEEF BULALO, STEAMED RICE]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import fpmax\n",
    "transactions = [\n",
    "    x for x in df_pos.groupby('BILL_ID')['ITEM_ID']\n",
    "    .apply(lambda x: list(x))\n",
    "]\n",
    "TE = TransactionEncoder()\n",
    "TE_fit = TE.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(TE_fit, columns = TE.columns_)\n",
    "df_out = (fpmax(df, min_support=0.01, use_colnames=True)\n",
    "          .sort_values(by='support', ascending=False)\n",
    "         )\n",
    "df_out['itemsets'] = df_out['itemsets'].apply(lambda x: tuple(sorted(x)))\n",
    "df_out['itemsets'] = (df_out['itemsets']\n",
    "                       .apply(lambda x: \n",
    "                              [df_pos.iloc[df_pos[df_pos['ITEM_ID']==y]\n",
    "                                           .drop_duplicates('ITEM_ID')\n",
    "                                           .index,2]\n",
    "                               .tolist()[0] for y in x]))\n",
    "\n",
    "df_out.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation 2 and 3\n",
    "\n",
    "Given the association rules, it is recommended to further promote their breakfast food in general. It is also recommended to bundle sunny eggs and juice with garlic rice. If the customer does not want juice, the store should recommend coffee as an alternative.  Lastly, it is recommended to bundle up any other breakfast items into a meal bundle that consists of \"ulam\", garlic rice, and a beverage and for non-breakfast items, create a meal bundle that consists of \"ulam\" and just plain steamed rice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:58:05.514756Z",
     "start_time": "2022-02-03T14:57:57.044626Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pos = pd.read_excel('pos_data.xlsx')\n",
    "transactions = df_pos.groupby('BILL_ID')['ITEM_ID'].apply(set)\n",
    "rules = fim.arules(transactions, supp = 1, conf = 60, report = 'l', eval = 'l' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:58:05.773412Z",
     "start_time": "2022-02-03T14:58:05.517100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedent</th>\n",
       "      <th>consequent</th>\n",
       "      <th>lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[WITH SUNNY EGG - COMP, WITH JUICE - COMP]</td>\n",
       "      <td>WITH GARLIC RICE - COMP</td>\n",
       "      <td>39.338718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[WITH SUNNY EGG - COMP, WITH COFFEE - COMP]</td>\n",
       "      <td>WITH GARLIC RICE - COMP</td>\n",
       "      <td>38.510067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[WITH GARLIC RICE - COMP, WITH JUICE - COMP]</td>\n",
       "      <td>WITH SUNNY EGG - COMP</td>\n",
       "      <td>37.843672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[WITH GARLIC RICE - COMP, FRIED DAING NA BANGU...</td>\n",
       "      <td>WITH SUNNY EGG - COMP</td>\n",
       "      <td>36.310278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[WITH GARLIC RICE - COMP, WITH COFFEE - COMP]</td>\n",
       "      <td>WITH SUNNY EGG - COMP</td>\n",
       "      <td>35.457490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[WITH GARLIC RICE - COMP]</td>\n",
       "      <td>WITH SUNNY EGG - COMP</td>\n",
       "      <td>34.245358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[WITH SUNNY EGG - COMP]</td>\n",
       "      <td>WITH GARLIC RICE - COMP</td>\n",
       "      <td>34.245358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[WITH GARLIC RICE - COMP, FRIED DAING NA BANGU...</td>\n",
       "      <td>WITH COFFEE - COMP</td>\n",
       "      <td>10.188673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[WITH GARLIC RICE - COMP, WITH SUNNY EGG - COMP]</td>\n",
       "      <td>WITH COFFEE - COMP</td>\n",
       "      <td>9.563746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[WITH GARLIC RICE - COMP]</td>\n",
       "      <td>WITH COFFEE - COMP</td>\n",
       "      <td>9.236804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[WITH SUNNY EGG - COMP]</td>\n",
       "      <td>WITH COFFEE - COMP</td>\n",
       "      <td>8.504631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[WITH GARLIC RICE - COMP, FRIED DAING NA BANGU...</td>\n",
       "      <td>WITH JUICE - COMP</td>\n",
       "      <td>7.195394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[SAUTEED CORNED BEEF - COMP, WITH COFFEE - COMP]</td>\n",
       "      <td>WITH JUICE - COMP</td>\n",
       "      <td>6.809533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[SWEET PORK TOCINO - COMP, WITH COFFEE - COMP]</td>\n",
       "      <td>WITH JUICE - COMP</td>\n",
       "      <td>6.745012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[WITH GARLIC RICE - COMP, WITH JUICE - COMP]</td>\n",
       "      <td>FRIED DAING NA BANGUS - COMP</td>\n",
       "      <td>5.673747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[WITH GARLIC RICE - COMP, WITH SUNNY EGG - COM...</td>\n",
       "      <td>FRIED DAING NA BANGUS - COMP</td>\n",
       "      <td>5.513672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[FRIED SEAFOOD PLATTER]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>3.452151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[FIESTA GRILL PLATTER]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>3.402581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[SINIGANG NA ISDA]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>3.365268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[PINAKBET]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>3.307324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[NATIVE CHICKEN TINOLA]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>3.300623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[BEEF BULALO]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>3.297775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[GRILLED TUNA BELLY]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>3.245214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[GRILLED TUNA BELLY]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>3.168141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[CRISPY PATA SA BAWANG]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>3.161718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[SEAFOOD CHOPSUEY]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>3.135263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[SIZZLING GARLIC CHICKEN]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>3.056170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[LECHON KAWALI]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>2.890424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[SIZZLING PORK SISIG]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>2.856325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[SEAFOOD-SALPICAO]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>2.833290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[CARAFFE PINOY ICED TEA]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>2.829713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[POMELO SHRIMP]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>2.796818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[PINOY KINILAW]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>2.772872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[GRILLED TUNA PANGA]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>2.716932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[SOUTHERN PORK RIBS]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>2.706191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[CRISPY CALAMARES]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>2.648148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[CAPT. TUNA SISIG]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>2.643968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[BUFFALO CHICKEN WINGS]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>2.624877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[CARAFFE LEMON CUCUMBER]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>2.493140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[COUNTRY FRIED CHICKEN]</td>\n",
       "      <td>STEAMED RICE</td>\n",
       "      <td>2.402673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           antecedent  \\\n",
       "0          [WITH SUNNY EGG - COMP, WITH JUICE - COMP]   \n",
       "1         [WITH SUNNY EGG - COMP, WITH COFFEE - COMP]   \n",
       "2        [WITH GARLIC RICE - COMP, WITH JUICE - COMP]   \n",
       "3   [WITH GARLIC RICE - COMP, FRIED DAING NA BANGU...   \n",
       "4       [WITH GARLIC RICE - COMP, WITH COFFEE - COMP]   \n",
       "5                           [WITH GARLIC RICE - COMP]   \n",
       "6                             [WITH SUNNY EGG - COMP]   \n",
       "7   [WITH GARLIC RICE - COMP, FRIED DAING NA BANGU...   \n",
       "8    [WITH GARLIC RICE - COMP, WITH SUNNY EGG - COMP]   \n",
       "9                           [WITH GARLIC RICE - COMP]   \n",
       "10                            [WITH SUNNY EGG - COMP]   \n",
       "11  [WITH GARLIC RICE - COMP, FRIED DAING NA BANGU...   \n",
       "12   [SAUTEED CORNED BEEF - COMP, WITH COFFEE - COMP]   \n",
       "13     [SWEET PORK TOCINO - COMP, WITH COFFEE - COMP]   \n",
       "14       [WITH GARLIC RICE - COMP, WITH JUICE - COMP]   \n",
       "15  [WITH GARLIC RICE - COMP, WITH SUNNY EGG - COM...   \n",
       "16                            [FRIED SEAFOOD PLATTER]   \n",
       "17                             [FIESTA GRILL PLATTER]   \n",
       "18                                 [SINIGANG NA ISDA]   \n",
       "19                                         [PINAKBET]   \n",
       "20                            [NATIVE CHICKEN TINOLA]   \n",
       "21                                      [BEEF BULALO]   \n",
       "22                               [GRILLED TUNA BELLY]   \n",
       "23                               [GRILLED TUNA BELLY]   \n",
       "24                            [CRISPY PATA SA BAWANG]   \n",
       "25                                 [SEAFOOD CHOPSUEY]   \n",
       "26                          [SIZZLING GARLIC CHICKEN]   \n",
       "27                                    [LECHON KAWALI]   \n",
       "28                              [SIZZLING PORK SISIG]   \n",
       "29                                 [SEAFOOD-SALPICAO]   \n",
       "30                           [CARAFFE PINOY ICED TEA]   \n",
       "31                                    [POMELO SHRIMP]   \n",
       "32                                    [PINOY KINILAW]   \n",
       "33                               [GRILLED TUNA PANGA]   \n",
       "34                               [SOUTHERN PORK RIBS]   \n",
       "35                                 [CRISPY CALAMARES]   \n",
       "36                                 [CAPT. TUNA SISIG]   \n",
       "37                            [BUFFALO CHICKEN WINGS]   \n",
       "38                           [CARAFFE LEMON CUCUMBER]   \n",
       "39                            [COUNTRY FRIED CHICKEN]   \n",
       "\n",
       "                      consequent       lift  \n",
       "0        WITH GARLIC RICE - COMP  39.338718  \n",
       "1        WITH GARLIC RICE - COMP  38.510067  \n",
       "2          WITH SUNNY EGG - COMP  37.843672  \n",
       "3          WITH SUNNY EGG - COMP  36.310278  \n",
       "4          WITH SUNNY EGG - COMP  35.457490  \n",
       "5          WITH SUNNY EGG - COMP  34.245358  \n",
       "6        WITH GARLIC RICE - COMP  34.245358  \n",
       "7             WITH COFFEE - COMP  10.188673  \n",
       "8             WITH COFFEE - COMP   9.563746  \n",
       "9             WITH COFFEE - COMP   9.236804  \n",
       "10            WITH COFFEE - COMP   8.504631  \n",
       "11             WITH JUICE - COMP   7.195394  \n",
       "12             WITH JUICE - COMP   6.809533  \n",
       "13             WITH JUICE - COMP   6.745012  \n",
       "14  FRIED DAING NA BANGUS - COMP   5.673747  \n",
       "15  FRIED DAING NA BANGUS - COMP   5.513672  \n",
       "16                  STEAMED RICE   3.452151  \n",
       "17                  STEAMED RICE   3.402581  \n",
       "18                  STEAMED RICE   3.365268  \n",
       "19                  STEAMED RICE   3.307324  \n",
       "20                  STEAMED RICE   3.300623  \n",
       "21                  STEAMED RICE   3.297775  \n",
       "22                  STEAMED RICE   3.245214  \n",
       "23                  STEAMED RICE   3.168141  \n",
       "24                  STEAMED RICE   3.161718  \n",
       "25                  STEAMED RICE   3.135263  \n",
       "26                  STEAMED RICE   3.056170  \n",
       "27                  STEAMED RICE   2.890424  \n",
       "28                  STEAMED RICE   2.856325  \n",
       "29                  STEAMED RICE   2.833290  \n",
       "30                  STEAMED RICE   2.829713  \n",
       "31                  STEAMED RICE   2.796818  \n",
       "32                  STEAMED RICE   2.772872  \n",
       "33                  STEAMED RICE   2.716932  \n",
       "34                  STEAMED RICE   2.706191  \n",
       "35                  STEAMED RICE   2.648148  \n",
       "36                  STEAMED RICE   2.643968  \n",
       "37                  STEAMED RICE   2.624877  \n",
       "38                  STEAMED RICE   2.493140  \n",
       "39                  STEAMED RICE   2.402673  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lift = (pd.DataFrame(sorted(rules, key=lambda x: -x[2]))\n",
    "           .rename({0:'consequent', 1:'antecedent', 2:'lift'}, axis=1)\n",
    "           [['antecedent', 'consequent', 'lift']]\n",
    "          )\n",
    "df_lift['antecedent'] = (df_lift['antecedent']\n",
    "                         .apply(lambda x: [df_pos\n",
    "                                           .iloc[df_pos[df_pos['ITEM_ID']==y]\n",
    "                                                 .drop_duplicates('ITEM_ID')\n",
    "                                                 .index,2]\n",
    "                                           .tolist()[0] for y in x]\n",
    "                               )\n",
    "                        )\n",
    "df_lift['consequent'] = (df_lift['consequent']\n",
    "                         .apply(lambda x: \n",
    "                                df_pos\n",
    "                                .iloc[df_pos[df_pos['ITEM_ID']==x]\n",
    "                                      .drop_duplicates('ITEM_ID')\n",
    "                                      .index,2].tolist()[0]\n",
    "                               )\n",
    "                        )\n",
    "\n",
    "df_lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c35bdfea2e3083f905ea904036fec06a",
     "grade": false,
     "grade_id": "cell-4b8efe45c9599102",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For this problem, you will work with the first 50000 lines of the [Book-Crossing](http://www2.informatik.uni-freiburg.de/~cziegler/BX) dataset (`/mnt/data/public/book-crossing/BX-Book-Ratings.csv`). The file itself follows `latin1` encoding. Treat zero as explicit ratings. When sorting, set `kind='mergesort'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2a [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6c92358523024a27f5f0fbfa7bbd1f7",
     "grade": false,
     "grade_id": "cell-2f5553d189f3ca4a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Create a function `user_recommend` that accepts a user ID and returns the 10 best `ISBN` recommendations for that `user` by a user-based recommender system using $k=31$ nearest neighbors. Sort by decreasing estimated rating then by lexicographic order of the ISBN code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:58:05.784558Z",
     "start_time": "2022-02-03T14:58:05.775867Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3aaaaddad0956d9f381a60e80bf90475",
     "grade": false,
     "grade_id": "cell-19e631ca700f3532",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def user_recommend(U):\n",
    "    n=31\n",
    "#     U=99\n",
    "    df_utility = (pd.read_csv('/mnt/data/public/book-crossing/BX-Book-Ratings.csv',\n",
    "                              nrows=50_000,\n",
    "                              encoding='latin1',\n",
    "                              sep=';')\n",
    "                  .pivot(index='User-ID', columns='ISBN', values='Book-Rating'))\n",
    "    df_out = df_utility.copy()\n",
    "    df_centered = df_utility.apply(lambda x: x-x.mean(), axis=1)\n",
    "\n",
    "\n",
    "    df_others = df_centered.drop(U)\n",
    "    items_to_predict =df_centered.columns.difference(df_centered.loc[U].dropna().index)\n",
    "\n",
    "    df_filtered = df_centered.loc[:, df_centered.columns.difference(items_to_predict)].dropna(axis='rows', how='all')\n",
    "\n",
    "    d={}\n",
    "\n",
    "    from scipy.spatial.distance import cosine\n",
    "    for o in tqdm.tqdm_notebook(df_filtered.drop(U).index):\n",
    "        df = df_centered.loc[[U,o]].dropna(axis='columns')\n",
    "        d[o] = 1 - cosine(df.loc[U], df.loc[o])\n",
    "\n",
    "    top_n_users = pd.Series(d).sort_values(kind='mergesort', ascending=(False))[:n]\n",
    "\n",
    "\n",
    "    for items in tqdm.tqdm_notebook(items_to_predict):\n",
    "        s_ratings = df_centered.loc[top_n_users.index, items].dropna()\n",
    "        s_dist = top_n_users[s_ratings.index]\n",
    "        df_out.loc[U,items] = ((s_ratings * (s_dist)).sum()/(s_dist).sum()) + df_utility.loc[U].mean()\n",
    "\n",
    "    df_preds = df_out.loc[U, items_to_predict].to_frame().reset_index().sort_values([99, 'ISBN'], kind='mergesort', ascending=(False,True))\n",
    "    return df_preds['ISBN'].tolist()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:58:53.350927Z",
     "start_time": "2022-02-03T14:58:05.786638Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "578344d1d167bde555f069c85fb30c38",
     "grade": true,
     "grade_id": "cell-84cb1c1db55410ef",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29961/2894367821.py:21: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for o in tqdm.tqdm_notebook(df_filtered.drop(U).index):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f188e2b1f24224b9db4b2983857769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/scipy/spatial/distance.py:699: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "/tmp/ipykernel_29961/2894367821.py:28: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for items in tqdm.tqdm_notebook(items_to_predict):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5427c5d210294c20b574e828e0b14606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29961/2894367821.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  df_out.loc[U,items] = ((s_ratings * (s_dist)).sum()/(s_dist).sum()) + df_utility.loc[U].mean()\n",
      "/tmp/ipykernel_29961/2894367821.py:31: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  df_out.loc[U,items] = ((s_ratings * (s_dist)).sum()/(s_dist).sum()) + df_utility.loc[U].mean()\n"
     ]
    }
   ],
   "source": [
    "recos_user = user_recommend(99)\n",
    "\n",
    "assert_equal(\n",
    "    recos_user[:5], \n",
    "    ['0060090367', '0060922532', '0060934425', '0060956445', '006109286X'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2b [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "866cc8aa5dd311cf829edae6abbcf131",
     "grade": false,
     "grade_id": "cell-d0961f2e6b167d12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Create a function `item_recommend` that accepts a user ID and returns the 10 best ISBN recommendations for that user by an item-based recommender system using $k=25$ nearest neighbors. Sort the columns of the utility matrix lexicographically. Consider only the 13000th to 14000th (exclusive) items/columns (index starts from 0). Recommend only items from the first 100 columns (items) of the resulting reduced utility matrix. Sort by decreasing estimated rating then by lexicographic order of the ISBN code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:58:57.705478Z",
     "start_time": "2022-02-03T14:58:53.353240Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29961/2220838927.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for item in tqdm.tqdm_notebook(items_to_predict):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d05941281744c6ba3f45073c36e0aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "n=25\n",
    "U=11676\n",
    "df_utility = (pd.read_csv('/mnt/data/public/book-crossing/BX-Book-Ratings.csv',\n",
    "                          nrows=50_000,\n",
    "                          encoding='latin1',\n",
    "                          sep=';')\n",
    "              .pivot(index='User-ID', columns='ISBN', values='Book-Rating')\n",
    "              )\n",
    "\n",
    "df_utility = df_utility[pd.Series(df_utility.columns).sort_values(kind='mergesort').tolist()[13000:14000]]\n",
    "df_out = df_utility.copy()\n",
    "\n",
    "### user mean centering\n",
    "df_centered = df_utility.apply(lambda x: x-x.mean(), axis=1)\n",
    "\n",
    "items_to_predict = df_utility.iloc[:,:100].columns.intersection(df_utility.loc[U].drop(df_utility.loc[U].dropna().index).index)\n",
    "\n",
    "for item in tqdm.tqdm_notebook(items_to_predict):\n",
    "\n",
    "    df_filtered = df_centered.loc[df_centered.loc[:, [item]].dropna(axis='rows').index,:].dropna(axis='columns', how='all')\n",
    "\n",
    "\n",
    "    d = {}\n",
    "    if len(df_filtered.columns.difference([item])) == 0:\n",
    "        continue\n",
    "    for o in (df_filtered.columns.difference([item])):\n",
    "        df = df_centered.loc[:, [item, o]].dropna(axis='rows')\n",
    "        dist = 1-cosine(df[item], df[o])\n",
    "        d[o] = dist\n",
    "\n",
    "    top_n_items = pd.Series(d).sort_values(kind='mergesort', ascending=False)[:n]\n",
    "    ratings = df_centered.loc[U, top_n_items.index].dropna()\n",
    "    if len(ratings) == 0 :\n",
    "        continue\n",
    "\n",
    "    sim = top_n_items[ratings.index]\n",
    "    \n",
    "    ### user mean centering\n",
    "    try:\n",
    "        1/sum(sim)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        pred_i = (sum(ratings*sim)/(sum(sim))) + df_utility.loc[U,:].mean()\n",
    "    \n",
    "    df_out.loc[U, item] = pred_i\n",
    "#     return df_out.loc[U, items_to_predict].sort_values(kind='mergesort', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:58:57.715024Z",
     "start_time": "2022-02-03T14:58:57.707416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ISBN\n",
       "0425176339    13.000000\n",
       "0425178773     9.000000\n",
       "0425177807     8.500000\n",
       "0425183009     8.500000\n",
       "0425182142     8.250000\n",
       "0425181928     8.142857\n",
       "0425179265     8.000000\n",
       "0425177351     6.250000\n",
       "0425181863     6.000000\n",
       "0425177424     5.200000\n",
       "0425183270     5.200000\n",
       "0425181111     5.083578\n",
       "0425182673     5.000000\n",
       "0425183750     4.500000\n",
       "0425180034     4.000000\n",
       "0425180298     4.000000\n",
       "0425181200     4.000000\n",
       "0425181979     4.000000\n",
       "0425182932     4.000000\n",
       "0425183025     4.000000\n",
       "0425176037     3.875000\n",
       "0425180042     3.800000\n",
       "0425181480     3.333333\n",
       "0425177009     2.615385\n",
       "042518045X     2.571429\n",
       "0425178234     2.000000\n",
       "0425176053     0.000000\n",
       "0425183181     0.000000\n",
       "042518269X    -1.500000\n",
       "0425180905    -4.000000\n",
       "0425175456    -8.000000\n",
       "0425176428          NaN\n",
       "0425176614          NaN\n",
       "0425176673          NaN\n",
       "0425176789          NaN\n",
       "0425176940          NaN\n",
       "0425177696          NaN\n",
       "0425178242          NaN\n",
       "0425178552          NaN\n",
       "0425178781          NaN\n",
       "0425178838          NaN\n",
       "0425178854          NaN\n",
       "0425179206          NaN\n",
       "0425179648          NaN\n",
       "0425180468          NaN\n",
       "0425181618          NaN\n",
       "0425181839          NaN\n",
       "0425182150          NaN\n",
       "0425182215          NaN\n",
       "042518238X          NaN\n",
       "0425182843          NaN\n",
       "0425183238          NaN\n",
       "0425183653          NaN\n",
       "Name: 11676, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out.loc[U, items_to_predict].sort_values(kind='mergesort', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:58:57.847541Z",
     "start_time": "2022-02-03T14:58:57.716596Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95f3b937984a7a7a206f7d7839613fa3",
     "grade": false,
     "grade_id": "cell-e8a4eed633effb99",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def item_recommend(U):\n",
    "    from scipy.spatial.distance import cosine\n",
    "    n=25\n",
    "#     U=11676\n",
    "    ### reading file and converting to user-item utility matrix\n",
    "    df_utility = (pd.read_csv('/mnt/data/public/book-crossing/BX-Book-Ratings.csv',\n",
    "                              nrows=50_000,\n",
    "                              encoding='latin1',\n",
    "                              sep=';')\n",
    "                  .pivot(index='User-ID', columns='ISBN', values='Book-Rating')\n",
    "                  )\n",
    "    ### Including only 13k-th column up to 13999th column once it is lexicographically sorted\n",
    "    df_utility = df_utility[pd.Series(df_utility.columns)\n",
    "                            .sort_values(kind='mergesort')\n",
    "                            .tolist()[13000:14000]]\n",
    "    ### copying utility matrix to input predictions later\n",
    "    df_out = df_utility.copy()\n",
    "    ### user mean centering\n",
    "    df_centered = df_utility.apply(lambda x: x-x.mean(), axis=1)\n",
    "    ### Knowing what items to predict for user U\n",
    "    items_to_predict = (df_utility\n",
    "                        .iloc[:,:100]\n",
    "                        .columns\n",
    "                        .intersection(df_utility\n",
    "                                      .loc[U]\n",
    "                                      .drop(df_utility.loc[U]\n",
    "                                            .dropna()\n",
    "                                            .index)\n",
    "                                      .index)\n",
    "                       )\n",
    "    ### Looping through items to predict\n",
    "    for item in tqdm.tqdm_notebook(items_to_predict):\n",
    "        ### knowing which items can be used to calculate cosine distance with variable \"item\"\n",
    "        df_filtered = (df_centered\n",
    "                       .loc[df_centered\n",
    "                            .loc[:, [item]]\n",
    "                            .dropna(axis='rows')\n",
    "                            .index,:]\n",
    "                       .dropna(axis='columns', how='all'))\n",
    "        d = {}\n",
    "        ### sometimes there are no items that can be cosine distance'd with var \"item\"\n",
    "        if len(df_filtered.columns.difference([item])) == 0:\n",
    "            continue\n",
    "        ### if there are items that can, loop through them and calculate cosine similarity which is 1-cosdist    \n",
    "        for o in (df_filtered.columns.difference([item])):\n",
    "            df = df_centered.loc[:, [item, o]].dropna(axis='rows')\n",
    "            dist = 1-cosine(df[item], df[o])\n",
    "            d[o] = dist\n",
    "        ### sorting top n items that are most similar with var \"item\"\n",
    "        top_n_items = (pd.Series(d)\n",
    "                       .sort_values(kind='mergesort', ascending=False)[:n])\n",
    "        ### ratings of user U for the top n items\n",
    "        ratings = df_centered.loc[U, top_n_items.index].dropna()\n",
    "        ### sometimes user U hasn't rated top_n_items yet\n",
    "        if len(ratings) == 0 :\n",
    "            continue\n",
    "        ### getting cosine similarities of corresponding top n items that user U has rated before\n",
    "        sim = top_n_items[ratings.index]\n",
    "\n",
    "        ### check if sum of cosine similarities is non-zero. If zero, then skip\n",
    "        try:\n",
    "            1/sum(sim)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        #### prediction for var \"item\" = calculate weighted average + user mean to reverse user-mean-centering. Weights are the cosine similarities\n",
    "        else:\n",
    "            pred_i = ((sum(ratings*sim)/(sum(sim))) \n",
    "                      + df_utility.loc[U,:].mean())\n",
    "        df_out.loc[U, item] = pred_i\n",
    "    return (df_out\n",
    "            .loc[U, items_to_predict]\n",
    "            .sort_values(kind='mergesort', ascending=False)\n",
    "            .index.tolist()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:59:02.241308Z",
     "start_time": "2022-02-03T14:58:57.851703Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a81cc7d21783b0a4ccb484305cd4cfc",
     "grade": true,
     "grade_id": "cell-942cb5f54adac3a4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29961/453521321.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for item in tqdm.tqdm_notebook(items_to_predict):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4186b3e5beea485fad2d9cee2e4c59a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recos_item = item_recommend(11676)\n",
    "\n",
    "assert_equal(\n",
    "    recos_item[:5],\n",
    "    ['0425176339', '0425178773', '0425177807', '0425183009', '0425182142']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "967d5b152159590f377b62243de94f29",
     "grade": false,
     "grade_id": "cell-8aee69289eadb740",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Create a function `svd_recommend` that accepts a user ID and returns the 10 best ISBN recommendations for that user by an SVD latent factor model recommender system. Use surprise to implement `svd_recommend` with 100 factors, 20 epochs, biased `True` and random_state `1337`. Consider all of `BX-Book-Ratings.csv` not just the first 50000 rows. Treat zero as an explicit rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T14:59:02.249784Z",
     "start_time": "2022-02-03T14:59:02.243317Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca01f46676736e638dda34082cf1f3bc",
     "grade": false,
     "grade_id": "cell-7561fd794040156e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def svd_recommend(user):\n",
    "    from surprise import Reader, Dataset, SVD\n",
    "    df = pd.read_csv('/mnt/data/public/book-crossing/BX-Book-Ratings.csv',\n",
    "                     encoding='latin-1', sep=';')\n",
    "\n",
    "    reader = Reader(rating_scale=(-10, 10))\n",
    "    data = Dataset.load_from_df(df, reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "\n",
    "    a = SVD(random_state=1337)\n",
    "    a.fit(trainset)\n",
    "\n",
    "    isbn = np.array(df['ISBN'].unique())\n",
    "    rated = np.array(df[df[\"User-ID\"] == user]['ISBN'].unique())\n",
    "    test = np.setdiff1d(isbn, rated)\n",
    "\n",
    "    testset = [[user, i, 3] for i in test]\n",
    "    predictions = a.test(testset)\n",
    "    pred_ratings = np.array([pred.est for pred in predictions])\n",
    "\n",
    "    top = pred_ratings.argsort()[::-1][:10]\n",
    "    return test[top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T15:02:36.106380Z",
     "start_time": "2022-02-03T14:59:02.251834Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64e1983e233a938cd195002fdcc184c5",
     "grade": true,
     "grade_id": "cell-e613599554f2871f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "recos_svd = svd_recommend(198711)\n",
    "\n",
    "assert_equal(\n",
    "    set(recos_svd[:6]),\n",
    "    {'0006513220',\n",
    "     '0060248025',\n",
    "     '0615116426',\n",
    "     '1844262553',\n",
    "     '8445071416',\n",
    "     '8826703132'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
