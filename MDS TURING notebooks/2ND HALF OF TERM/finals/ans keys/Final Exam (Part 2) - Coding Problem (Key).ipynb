{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics for Data Science\n",
    "\n",
    "## Final Exam (Part 1) - Coding Problem\n",
    "\n",
    "This notebook should be submitted <b><u>individually</u></b>.   \n",
    "\n",
    "Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Read the instructions and questions carefully.</u></b>\n",
    "\n",
    "Do <b><u>NOT</u></b> import any other libraries aside from those below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T09:38:19.788176Z",
     "start_time": "2021-05-10T09:38:19.642015Z"
    },
    "execution": {
     "iopub.execute_input": "2021-06-13T08:20:54.207385Z",
     "iopub.status.busy": "2021-06-13T08:20:54.206925Z",
     "iopub.status.idle": "2021-06-13T08:20:54.227867Z",
     "shell.execute_reply": "2021-06-13T08:20:54.227000Z",
     "shell.execute_reply.started": "2021-06-13T08:20:54.207333Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the diabetes dataset\n",
    "x, y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Use three features\n",
    "x = x[:, [2, 3, 9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fitting a Ridge Regression Using Gradient Descent (10 pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement batch gradient descent to fit the diabetes dataset to a multiple linear regression model,\n",
    "\n",
    "\\begin{align*}\n",
    "    y &= \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3\n",
    "\\end{align*}\n",
    "\n",
    "with squared loss (not MSE loss, but notice how they are equivalent) and ridge regularization (also known as L2 regularization),\n",
    "\n",
    "\\begin{align*}\n",
    "    J(\\hat{\\theta}_0, \\hat{\\theta}_1, \\hat{\\theta}_2, \\hat{\\theta}_3) &= \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{3} (\\hat{\\theta}_j)^2\n",
    "\\end{align*}\n",
    "\n",
    "Note that $\\hat{\\theta}_0$ is NOT included in the penalty term.\n",
    "\n",
    "---\n",
    "\n",
    "### Interlude\n",
    "\n",
    "One of the goals of ridge regression (and regularization in general) is to reduce \"overfitting\". For linear regression, this is especially important when the features are highly correlated with each other.\n",
    "\n",
    "From a classical statistics perspective, it turns out that we can decompose the MSE statistic into <b><i>bias</b></i> and <b><i>variance</b></i> terms. Ridge regularization allows us to achieve a lower MSE by introducing a \"slight\" amount of bias in order to get a lower variance.\n",
    "\n",
    "We will revisit this concept in ACS and you will be using Ridge (and Lasso) as a regularization tool in ML! Very cool.\n",
    "\n",
    "---\n",
    "\n",
    "For this problem, set the regularization parameter $\\lambda = 1$.\n",
    "\n",
    "Print a diagnostic output for the first five iterations ($k = 0, 1, 2, 3, 4$), then print the output every 1000 iterations until convergence ($k = 1000, 2000, 3000, \\ldots$).\n",
    "\n",
    "The diagnostic output should contain the following: the iteration $k$, parameters $\\hat{\\theta}_{k}$, and loss $J$.\n",
    "\n",
    "Your solution should be <u>stable</u> and <u>exact</u> up to 4 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T08:53:38.883820Z",
     "iopub.status.busy": "2021-06-13T08:53:38.883331Z",
     "iopub.status.idle": "2021-06-13T08:53:39.400318Z",
     "shell.execute_reply": "2021-06-13T08:53:39.399774Z",
     "shell.execute_reply.started": "2021-06-13T08:53:38.883764Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.0010\n",
      "k = 0 - theta_k[0] = 1.0000 - theta_k[1] = 1.0000 - theta_k[2] = 1.0000 - theta_k[3] = 1.0000 - J = 12712318.5496\n",
      "k = 1 - theta_k[0] = 134.6020 - theta_k[1] = 2.8933 - theta_k[2] = 2.4239 - theta_k[3] = 2.2329 - J = 2745188.1244\n",
      "k = 2 - theta_k[0] = 150.0998 - theta_k[1] = 4.7769 - theta_k[2] = 3.8397 - theta_k[3] = 3.4583 - J = 2604131.4706\n",
      "k = 3 - theta_k[0] = 151.8976 - theta_k[1] = 6.6510 - theta_k[2] = 5.2473 - theta_k[3] = 4.6762 - J = 2595370.4540\n",
      "k = 4 - theta_k[0] = 152.1061 - theta_k[1] = 8.5155 - theta_k[2] = 6.6469 - theta_k[3] = 5.8866 - J = 2588464.7590\n",
      "k = 1000 - theta_k[0] = 152.1335 - theta_k[1] = 384.6549 - theta_k[2] = 244.0755 - theta_k[3] = 188.5413 - J = 1960916.9497\n",
      "k = 2000 - theta_k[0] = 152.1335 - theta_k[1] = 390.1390 - theta_k[2] = 243.9078 - theta_k[3] = 186.2808 - J = 1960852.2453\n",
      "k = 3000 - theta_k[0] = 152.1335 - theta_k[1] = 390.3207 - theta_k[2] = 243.8628 - theta_k[3] = 186.1551 - J = 1960852.1567\n",
      "k = 4000 - theta_k[0] = 152.1335 - theta_k[1] = 390.3278 - theta_k[2] = 243.8609 - theta_k[3] = 186.1500 - J = 1960852.1565\n",
      "k = 5000 - theta_k[0] = 152.1335 - theta_k[1] = 390.3281 - theta_k[2] = 243.8608 - theta_k[3] = 186.1498 - J = 1960852.1565\n"
     ]
    }
   ],
   "source": [
    "# Use the following variables\n",
    "theta_k = np.array([1, 1, 1, 1])\n",
    "lam = 1\n",
    "lr = 0.001\n",
    "\n",
    "# Your code here\n",
    "\n",
    "def J(theta, x, y, lam):\n",
    "    return np.sum((y - (theta[0] + theta[1]*x[:, 0] + theta[2]*x[:, 1] + theta[3]*x[:, 2]))**2) + lam*(np.sum(theta**2) - theta[0]**2) # Remove theta_0**2\n",
    "\n",
    "def J_0(theta, x, y, lam):\n",
    "    return -2*np.sum(y - (theta[0] + theta[1]*x[:, 0] + theta[2]*x[:, 1] + theta[3]*x[:, 2])) #+ 2*lam*theta[0]\n",
    "\n",
    "def J_1(theta, x, y, lam):\n",
    "    return -2*np.sum((y - (theta[0] + theta[1]*x[:, 0] + theta[2]*x[:, 1] + theta[3]*x[:, 2]))*x[:, 0]) + 2*lam*theta[1]\n",
    "\n",
    "def J_2(theta, x, y, lam):\n",
    "    return -2*np.sum((y - (theta[0] + theta[1]*x[:, 0] + theta[2]*x[:, 1] + theta[3]*x[:, 2]))*x[:, 1]) + 2*lam*theta[2]\n",
    "\n",
    "def J_3(theta, x, y, lam):\n",
    "    return -2*np.sum((y - (theta[0] + theta[1]*x[:, 0] + theta[2]*x[:, 1] + theta[3]*x[:, 2]))*x[:, 2]) + 2*lam*theta[3]\n",
    "\n",
    "print('lr = %.4f' %alpha)\n",
    "for k in range(0, 5001):\n",
    "    if k < 5:\n",
    "        print(\"k = %d -\" %k,\"theta_k[0] = %.4f -\" %theta_k[0],\"theta_k[1] = %.4f -\" %theta_k[1],\"theta_k[2] = %.4f -\" %theta_k[2],\"theta_k[3] = %.4f -\" %theta_k[3], \"J = %.4f\" %J(theta_k, x, y, lam))\n",
    "    elif k % 1000 == 0:\n",
    "        print(\"k = %d -\" %k,\"theta_k[0] = %.4f -\" %theta_k[0],\"theta_k[1] = %.4f -\" %theta_k[1],\"theta_k[2] = %.4f -\" %theta_k[2],\"theta_k[3] = %.4f -\" %theta_k[3], \"J = %.4f\" %J(theta_k, x, y, lam))\n",
    "    theta_k = theta_k - alpha*np.array([J_0(theta_k, x, y, lam), J_1(theta_k, x, y, lam), J_2(theta_k, x, y, lam), J_3(theta_k, x, y, lam)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T08:36:59.974903Z",
     "iopub.status.busy": "2021-06-13T08:36:59.974402Z",
     "iopub.status.idle": "2021-06-13T08:36:59.984868Z",
     "shell.execute_reply": "2021-06-13T08:36:59.983765Z",
     "shell.execute_reply.started": "2021-06-13T08:36:59.974848Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [390.32812749 243.86077815 186.14983698]\n",
      "152.13348416289628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "clf = Ridge(alpha=1.0, fit_intercept=True)\n",
    "\n",
    "clf.fit(x, y)\n",
    "\n",
    "print('Coefficients: \\n', clf.coef_)\n",
    "\n",
    "print(clf.intercept_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
