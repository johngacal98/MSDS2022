{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVqGDiuylX4w"
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVqGDiuylX4w"
   },
   "source": [
    "## Bayes' Theorem\n",
    "\n",
    "Suppose that we wish to classify an observation into one of $K$ classes.\n",
    "\n",
    "Let $\\pi_k$ represent the overall *prior* probability that a randomly chosen observation comes from the $k$th class.\n",
    "\n",
    "Let $f_k(X=x) = \\Pr(X=x|Y=k)$ denote the *likelihood* that an observation $X$ comes from the $k$th class.\n",
    "\n",
    "Then **Bayes' theorem** states that the *posterior* probability is given by,\n",
    "\n",
    "$$\\Pr(Y=k|X=x) = \\frac{\\pi_k \\cdot f_k(x)}{\\sum_{i=1}^{K}\\pi_i \\cdot f_i(x)}$$\n",
    "\n",
    "In general, this model is called a **Bayes classifier**. Thus, for us to use this model we need to select a form for the likelihood function $f_k(X)$.\n",
    "\n",
    "Note that $X$ represents our predictor variables, meaning that $X$ can actually be a vector of features $X = [X_1, X_2, \\ldots, X_p]$. \n",
    "\n",
    "To \"select a form for our likelihood function\" means that we need to define the joint distribution over all our predictor random variables $[X_1, X_2, \\ldots, X_p]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVqGDiuylX4w"
   },
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "Suppose we assume that *within the $k$th class, the $p$ predictors are independent*, **(because of this assumption our model is super super fast to train)**\n",
    "\n",
    "$$f_k(x) = f_{k1}(x_1) \\cdot f_{k2}(x_2) \\cdots f_{kp}(x_p)$$\n",
    "\n",
    "where $f_{kj}$ is the density function of the $j$th predictor among observations in the $k$th class.\n",
    "\n",
    "Under this assumption, our classification model is called a **Naive Bayes classifier**. Thus our posterior probability is given by,\n",
    "\n",
    "$$\\Pr(Y=k|X=x) = \\frac{\\pi_k \\cdot f_{k1}(x_1) \\cdot f_{k2}(x_2) \\cdots f_{kp}(x_p)}{\\sum_{i=1}^{K}\\pi_i \\cdot f_{i1}(x_1) \\cdot f_{i2}(x_2) \\cdots f_{ip}(x_p)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVqGDiuylX4w"
   },
   "source": [
    "### Choosing a Model for $f_{kj}$\n",
    "\n",
    "If $X_j$ is numerical, we can assume that\n",
    "\n",
    "$$X_j|Y=k \\sim N(\\mu_{jk}, \\sigma_{jk}^2)$$\n",
    "\n",
    "In other words, we say that the $j$th predictor is drawn from a univariate normal distribution within each class $k$.\n",
    "\n",
    "If $X_j$ is categorical, then we can simply count the proportion of training observations for the $j$th predictor that corresponds to each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example - Naive Bayes with Mixed Data Types\n",
    "\n",
    "To understand what's happening, let's work through a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T06:52:37.946382Z",
     "iopub.status.busy": "2021-10-04T06:52:37.946382Z",
     "iopub.status.idle": "2021-10-04T06:52:37.973309Z",
     "shell.execute_reply": "2021-10-04T06:52:37.973309Z",
     "shell.execute_reply.started": "2021-10-04T06:52:37.946382Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.24</td>\n",
       "      <td>-5.79</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.88</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.72</td>\n",
       "      <td>-2.39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.73</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.65</td>\n",
       "      <td>-8.38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>10.77</td>\n",
       "      <td>-10.51</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>6.56</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>10.44</td>\n",
       "      <td>-2.32</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>3.80</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>16.98</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X1     X2  X3  y\n",
       "0   26.24  -5.79   2  1\n",
       "1    3.88   0.90   0  0\n",
       "2    4.72  -2.39   1  1\n",
       "3   -0.73  -1.63   0  0\n",
       "4   18.65  -8.38   0  0\n",
       "..    ...    ...  .. ..\n",
       "95  10.77 -10.51   2  0\n",
       "96   6.56  -1.59   0  0\n",
       "97  10.44  -2.32   0  1\n",
       "98   3.80  -0.76   2  0\n",
       "99  16.98   0.18   1  0\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('nb_toy.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we have 3 predictors (2 numerical, 1 categorical w/ 3 classes) and 2 target classes. \n",
    "\n",
    ">(In this case we dont OHE the categorical because it may not have significant impact on the performance of the model) -basti\n",
    "\n",
    "First, let's set up our Naive Bayes classifier by defining the priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T06:52:37.975304Z",
     "iopub.status.busy": "2021-10-04T06:52:37.975304Z",
     "iopub.status.idle": "2021-10-04T06:52:37.988269Z",
     "shell.execute_reply": "2021-10-04T06:52:37.988269Z",
     "shell.execute_reply.started": "2021-10-04T06:52:37.975304Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of classes\n",
    "k = 2\n",
    "\n",
    "# Prior probabilities (uniform)\n",
    "pi = np.ones(2)/2\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to estimate each of the $f_{kj}$. (j runs from 1 to 3 since we have 3 predictors)\n",
    "\n",
    "That's $f_{11}, f_{12}, f_{13}$  for $k=1$, and $f_{21}, f_{22}, f_{23}$ for $k=2$.\n",
    "\n",
    "So let's split the dataset into their respective classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T06:52:37.990265Z",
     "iopub.status.busy": "2021-10-04T06:52:37.989268Z",
     "iopub.status.idle": "2021-10-04T06:52:38.004258Z",
     "shell.execute_reply": "2021-10-04T06:52:38.004258Z",
     "shell.execute_reply.started": "2021-10-04T06:52:37.990265Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_1 = df[df['y'] == 0] # get data with label 0\n",
    "df_2 = df[df['y'] == 1] # get data with label 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate $f_{11}, f_{12}, f_{13}$. \n",
    "\n",
    "We should end up with 2 normal distributions and 1 categorical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T06:52:38.006227Z",
     "iopub.status.busy": "2021-10-04T06:52:38.006227Z",
     "iopub.status.idle": "2021-10-04T06:52:38.019216Z",
     "shell.execute_reply": "2021-10-04T06:52:38.019216Z",
     "shell.execute_reply.started": "2021-10-04T06:52:38.006227Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_11 mean = 11.12\n",
      "f_11 std = 9.6\n",
      "f_12 mean = -3.72\n",
      "f_12 std = 3.92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.47058824, 0.29411765, 0.23529412])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f_11 (this is not the actual function, we plug the mean and stdev in the gaussian formula)\n",
    "mean_11 = np.mean(df_1['X1']) # this is ur MLE estimate \n",
    "std_11 = np.std(df_1['X1'])\n",
    "\n",
    "print('f_11 mean =', round(mean_11,2))\n",
    "print('f_11 std =', round(std_11,2))\n",
    "\n",
    "# f_12\n",
    "mean_12 = np.mean(df_1['X2'])\n",
    "std_12 = np.std(df_1['X2'])\n",
    "\n",
    "print('f_12 mean =', round(mean_12,2))\n",
    "print('f_12 std =', round(std_12,2))\n",
    "\n",
    "# f_13 (probability vector)\n",
    "f_13 = (df_1['X3'].value_counts()/df_1['X3'].shape[0]).to_numpy()\n",
    "f_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T06:52:38.020190Z",
     "iopub.status.busy": "2021-10-04T06:52:38.020190Z",
     "iopub.status.idle": "2021-10-04T06:52:38.035173Z",
     "shell.execute_reply": "2021-10-04T06:52:38.035173Z",
     "shell.execute_reply.started": "2021-10-04T06:52:38.020190Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.116617647058824"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate $f_{21}, f_{22}, f_{23}$. Which is similar to what we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T06:52:38.036147Z",
     "iopub.status.busy": "2021-10-04T06:52:38.036147Z",
     "iopub.status.idle": "2021-10-04T06:52:38.050141Z",
     "shell.execute_reply": "2021-10-04T06:52:38.050141Z",
     "shell.execute_reply.started": "2021-10-04T06:52:38.036147Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_21 mean = 9.52\n",
      "f_21 std = 6.89\n",
      "f_22 mean = -2.68\n",
      "f_22 std = 3.18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.4375, 0.4375, 0.125 ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f_21\n",
    "mean_21 = np.mean(df_2['X1'])\n",
    "std_21 = np.std(df_2['X1'])\n",
    "\n",
    "print('f_21 mean =', round(mean_21,2))\n",
    "print('f_21 std =', round(std_21,2))\n",
    "\n",
    "# f_22\n",
    "mean_22 = np.mean(df_2['X2'])\n",
    "std_22 = np.std(df_2['X2'])\n",
    "\n",
    "print('f_22 mean =', round(mean_22,2))\n",
    "print('f_22 std =', round(std_22,2))\n",
    "\n",
    "# f_23\n",
    "f_23 = (df_2['X3'].value_counts()/df_2['X3'].shape[0]).to_numpy()\n",
    "f_23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a new observation $x^* = [x_1^*, x_2^*, x_3^*]$.\n",
    "\n",
    "Then using the Naive Bayes formula, the posterior probabilities of $x*$ belonging to class $k$ are given by the formula\n",
    "\n",
    "$$\\pi_k' = \\frac{\\pi_k \\cdot f_{k1}(x_1^*) \\cdot f_{k2}(x_2^*) \\cdot f_{k3}(x_3^*)}{(\\pi_1 \\cdot f_{11}(x_1^*) \\cdot f_{12}(x_2^*) \\cdot f_{13}(x_3^*)) + (\\pi_2 \\cdot f_{21}(x_1^*) \\cdot f_{22}(x_2^*) \\cdot f_{23}(x_3^*))}$$\n",
    "\n",
    "for $k=1,2$.\n",
    "\n",
    "As an example, let $x^* = [8, -1, 0]$. Let's calculate the posterior probabilities for $x^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T06:52:38.052103Z",
     "iopub.status.busy": "2021-10-04T06:52:38.051106Z",
     "iopub.status.idle": "2021-10-04T06:52:38.066091Z",
     "shell.execute_reply": "2021-10-04T06:52:38.066091Z",
     "shell.execute_reply.started": "2021-10-04T06:52:38.051106Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3545775 0.6454225]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# New data point\n",
    "x_new = np.array([8, -1, 0])\n",
    "\n",
    "# Store posteriors here\n",
    "pi_post = np.zeros(2) # for storage\n",
    "\n",
    "# Calculate posteriors\n",
    "prior1_lik1 = pi[0] * norm.pdf(x_new[0], mean_11, std_11) * norm.pdf(x_new[1], mean_12, std_12) * f_13[x_new[2]]\n",
    "prior2_lik2 = pi[1] * norm.pdf(x_new[0], mean_21, std_21) * norm.pdf(x_new[1], mean_22, std_22) * f_23[x_new[2]]\n",
    "\n",
    "pi_post[0] = prior1_lik1/(prior1_lik1 + prior2_lik2)\n",
    "pi_post[1] = prior2_lik2/(prior1_lik1 + prior2_lik2)\n",
    "\n",
    "print(pi_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T06:52:38.067067Z",
     "iopub.status.busy": "2021-10-04T06:52:38.067067Z",
     "iopub.status.idle": "2021-10-04T06:52:38.082048Z",
     "shell.execute_reply": "2021-10-04T06:52:38.082048Z",
     "shell.execute_reply.started": "2021-10-04T06:52:38.067067Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03943492349767363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.039434923497673635"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gaussian_pdf(x, mu, sigma):\n",
    "    return (1/np.sqrt(2*np.pi*sigma**2))*np.exp(-(x-mu)**2/(2*sigma**2))\n",
    "\n",
    "print(gaussian_pdf(x_new[0], mean_11, std_11))\n",
    "norm.pdf(x_new[0], mean_11, std_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we would classify this new observation as $k=2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify using `sklearn` (Numeric Data Types)\n",
    "\n",
    "Unfortunately, `sklearn` does not directly support mixed data types. However, we can still write code to adapt it for such.\n",
    "\n",
    "Before that, let's see if we can replicate the posterior probabilities using only the numeric predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33807489 0.66192511]\n"
     ]
    }
   ],
   "source": [
    "# Store posteriors here\n",
    "pi_post_num = np.zeros(2)\n",
    "\n",
    "# Calculate posteriors (since we are only dealing with numeric, we remove the categorical term aka the third one)\n",
    "prior1_lik1_num = pi[0] * norm.pdf(x_new[0], mean_11, std_11) * norm.pdf(x_new[1], mean_12, std_12)\n",
    "prior2_lik2_num = pi[1] * norm.pdf(x_new[0], mean_21, std_21) * norm.pdf(x_new[1], mean_22, std_22)\n",
    "\n",
    "pi_post_num[0] = prior1_lik1_num/(prior1_lik1_num + prior2_lik2_num)\n",
    "pi_post_num[1] = prior2_lik2_num/(prior1_lik1_num + prior2_lik2_num)\n",
    "\n",
    "print(pi_post_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33807489, 0.66192511]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X_num = df.iloc[:, :2]\n",
    "y = df.iloc[:, 3]\n",
    "\n",
    "model = GaussianNB(priors=pi).fit(X_num, y)\n",
    "model.predict_proba(np.array([[8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify using `sklearn` (Categorical Data Types)\n",
    "\n",
    "Now let's see if we can replicate the posterior probabilities using only the categorical predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51821862 0.48178138]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Store posteriors here\n",
    "pi_post_cat = np.zeros(2)\n",
    "\n",
    "# Calculate posteriors\n",
    "prior1_lik1_cat = pi[0] * f_13[x_new[2]]\n",
    "prior2_lik2_cat = pi[1] * f_23[x_new[2]]\n",
    "\n",
    "pi_post_cat[0] = prior1_lik1_cat/(prior1_lik1_cat + prior2_lik2_cat)\n",
    "pi_post_cat[1] = prior2_lik2_cat/(prior1_lik1_cat + prior2_lik2_cat)\n",
    "\n",
    "print(pi_post_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T06:52:38.130892Z",
     "iopub.status.busy": "2021-10-04T06:52:38.130892Z",
     "iopub.status.idle": "2021-10-04T06:52:38.145854Z",
     "shell.execute_reply": "2021-10-04T06:52:38.144882Z",
     "shell.execute_reply.started": "2021-10-04T06:52:38.130892Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JG\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.51821862, 0.48178138]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "X_cat = df.iloc[:, 2].to_numpy().reshape(-1, 1)\n",
    "y = df.iloc[:, 3]\n",
    "\n",
    "model = CategoricalNB(alpha=0, class_prior=pi).fit(X_cat, y) # alpha parameter controls laplace smoothing\n",
    "model.predict_proba(np.array([[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Laplace Smoothing\n",
    "\n",
    "\n",
    "Modify our original implementation to include a [Laplace smoothing](https://scikit-learn.org/stable/modules/naive_bayes.html#categorical-naive-bayes) parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T06:52:38.146882Z",
     "iopub.status.busy": "2021-10-04T06:52:38.145854Z",
     "iopub.status.idle": "2021-10-04T06:52:38.159816Z",
     "shell.execute_reply": "2021-10-04T06:52:38.159816Z",
     "shell.execute_reply.started": "2021-10-04T06:52:38.146882Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52027027, 0.47972973]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replicate the results using our own code + laplace smoothing \n",
    "model_replicate = CategoricalNB(alpha=1, class_prior=pi).fit(X_cat, y) # alpha parameter controls laplace smoothing\n",
    "model_replicate.predict_proba(np.array([[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T07:03:29.547653Z",
     "iopub.status.busy": "2021-10-04T07:03:29.547653Z",
     "iopub.status.idle": "2021-10-04T07:03:29.555632Z",
     "shell.execute_reply": "2021-10-04T07:03:29.555632Z",
     "shell.execute_reply.started": "2021-10-04T07:03:29.547653Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52027027 0.47972973]\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "n = 3\n",
    "\n",
    "f_13_lap = ((df_1['X3'].value_counts() + a)/(df_1['X3'].shape[0]+ (a*n))).to_numpy()\n",
    "f_23_lap = ((df_2['X3'].value_counts() + a)/(df_2['X3'].shape[0]+ (a*n))).to_numpy()\n",
    "\n",
    "# Store posteriors here\n",
    "pi_post_cat = np.zeros(2)\n",
    "# Calculate posteriors\n",
    "prior1_lik1_cat = pi[0] * f_13_lap[x_new[2]]\n",
    "prior2_lik2_cat = pi[1] * f_23_lap[x_new[2]]\n",
    "\n",
    "pi_post_cat[0] = prior1_lik1_cat/(prior1_lik1_cat + prior2_lik2_cat)\n",
    "pi_post_cat[1] = prior2_lik2_cat/(prior1_lik1_cat + prior2_lik2_cat)\n",
    "\n",
    "print(pi_post_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Naive Bayes Classifier with Mixed Data Types using `sklearn`\n",
    "\n",
    "Write code that uses `GaussianNB` and `CategoricalNB` to calculate the posterior probabilities for the mixed data type observation $x^* = [8, -1, 0]$. \n",
    "\n",
    "*Very cool version: Write the code as a general wrapper function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T07:08:59.472480Z",
     "iopub.status.busy": "2021-10-04T07:08:59.472480Z",
     "iopub.status.idle": "2021-10-04T07:08:59.490431Z",
     "shell.execute_reply": "2021-10-04T07:08:59.489441Z",
     "shell.execute_reply.started": "2021-10-04T07:08:59.472480Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gauss [[0.33807489 0.66192511]]\n",
      "cat [[0.51821862 0.48178138]]\n",
      "replicate [0.3545775 0.6454225]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JG\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X_num = df.iloc[:, :2]\n",
    "y = df.iloc[:, 3]\n",
    "\n",
    "model_gauss = GaussianNB(priors=pi).fit(X_num, y)\n",
    "gauss_prob = model_gauss.predict_proba(np.array([[8, -1]]))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "X_cat = df.iloc[:, 2].to_numpy().reshape(-1, 1)\n",
    "\n",
    "\n",
    "model_cat = CategoricalNB(alpha=0, class_prior=pi).fit(X_cat, y) # alpha parameter controls laplace smoothing\n",
    "cat_prob = model_cat.predict_proba(np.array([[0]]))\n",
    "print('gauss', gauss_prob)\n",
    "print('cat', cat_prob)\n",
    "print('replicate', pi_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T07:18:23.762463Z",
     "iopub.status.busy": "2021-10-04T07:18:23.760466Z",
     "iopub.status.idle": "2021-10-04T07:18:23.774461Z",
     "shell.execute_reply": "2021-10-04T07:18:23.774461Z",
     "shell.execute_reply.started": "2021-10-04T07:18:23.762463Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replicate [0.3545775 0.6454225]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.3545775, 0.6454225]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('replicate', pi_post)\n",
    "(gauss_prob * cat_prob) / np.sum((gauss_prob*cat_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `mixed data type` problem for Naive Bayes, you can combine predictions from gaussian and categorical NB of sklearn and then normalize the combination.\n",
    "\n",
    "multiply : combination of predictions<br>\n",
    "divide by np.sum : normalization"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO+AksVj6O1NtUXK3BVSw9H",
   "collapsed_sections": [],
   "mount_file_id": "1h9KphQD9lHl8On2417pLFfgXOdQiBbOt",
   "name": "ml2_arima_pt1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
