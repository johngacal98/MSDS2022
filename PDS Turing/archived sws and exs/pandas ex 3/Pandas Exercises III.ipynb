{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T08:08:50.130597Z",
     "start_time": "2021-05-28T08:08:49.716180Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c28a49a8f112386f36ec110c7ba7f1af",
     "grade": false,
     "grade_id": "cell-2af128eb8aabbb4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.testing import assert_equal, assert_allclose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0aab4cf07cc36bfc46400219892058c6",
     "grade": false,
     "grade_id": "cell-6afc28e57937b743",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "Create a function `double_work` that accepts a pandas data frame `df` with a time series index and columns `a` and `b`. It modifies `df` such that the value of `a` from 9am to 5pm (inclusive) of 1 Jan 2021 is double the value of `b` for those hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T02:32:13.201408Z",
     "start_time": "2021-05-30T02:32:13.195481Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edcf132ab8600b89eed09d5a378ae9fa",
     "grade": false,
     "grade_id": "cell-0acb5a75f001754f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def double_work(df):\n",
    "    \"\"\"\n",
    "    Modify DataFrame with time series index and columns a and b\n",
    "    \n",
    "    The value of column a from 9am to 5pm (inclusive) of 1 Jan 2021 is double \n",
    "    the value of column b for those hours.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame with column a and b with time series index\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: DataFrame\n",
    "        modified DataFrame\n",
    "    \"\"\"\n",
    "    newdf1 = df.loc[(df.index.hour >= 9) & (df.index.hour <= 17)].copy()\n",
    "\n",
    "    newdf1['a'] = newdf1['b']*2\n",
    "\n",
    "    df.update(newdf1)\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T02:46:18.660244Z",
     "start_time": "2021-05-30T02:46:18.624264Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40536697e7d33104194d21e677eeaa5b",
     "grade": true,
     "grade_id": "cell-17550f5d3c0e0259",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'a': range(24), \n",
    "        'b': np.linspace(1, 100, 24)\n",
    "    },\n",
    "        index=pd.date_range('2021-01-01 00:00', '2021-01-01 23:00', freq='H')\n",
    ")\n",
    "double_work(df)\n",
    "assert_equal(\n",
    "    df.index[:10].tolist(),\n",
    "    [pd.Timestamp('2021-01-01 00:00:00', freq='H'),\n",
    "     pd.Timestamp('2021-01-01 01:00:00', freq='H'),\n",
    "     pd.Timestamp('2021-01-01 02:00:00', freq='H'),\n",
    "     pd.Timestamp('2021-01-01 03:00:00', freq='H'),\n",
    "     pd.Timestamp('2021-01-01 04:00:00', freq='H'),\n",
    "     pd.Timestamp('2021-01-01 05:00:00', freq='H'),\n",
    "     pd.Timestamp('2021-01-01 06:00:00', freq='H'),\n",
    "     pd.Timestamp('2021-01-01 07:00:00', freq='H'),\n",
    "     pd.Timestamp('2021-01-01 08:00:00', freq='H'),\n",
    "     pd.Timestamp('2021-01-01 09:00:00', freq='H')]\n",
    ")\n",
    "assert_allclose(\n",
    "    df.iloc[:10].values,\n",
    "    [[ 0.        ,  1.        ],\n",
    "     [ 1.        ,  5.30434783],\n",
    "     [ 2.        ,  9.60869565],\n",
    "     [ 3.        , 13.91304348],\n",
    "     [ 4.        , 18.2173913 ],\n",
    "     [ 5.        , 22.52173913],\n",
    "     [ 6.        , 26.82608696],\n",
    "     [ 7.        , 31.13043478],\n",
    "     [ 8.        , 35.43478261],\n",
    "     [79.47826087, 39.73913043]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T09:23:46.381357Z",
     "start_time": "2021-05-29T09:23:46.377232Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e5935d5c058c86455a1f81a469847db",
     "grade": false,
     "grade_id": "cell-3da5244c27a4736e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 2\n",
    "\n",
    "Create a function `hourly_mem_usage` that reads `mem.csv` and returns the mean hourly memory usage in Philippine time (UTC+8) as a pandas time series. The first column of `mem.csv` is the time in UTC and the second column is the memory usage in bytes. Each interval is closed on the left but open on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T09:27:48.152541Z",
     "start_time": "2021-05-29T09:27:48.146147Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b075c9095095c08d290857cc01093018",
     "grade": false,
     "grade_id": "cell-2ab74f3afc7e2594",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def hourly_mem_usage():\n",
    "    \"\"\"\n",
    "    Compute mean hourly memory usage in UTC+8 time\n",
    "    \n",
    "    Read 'mem.csv' and return the mean hourly memory usage in Philippine\n",
    "    time (UTC+8) as a pandas time series. Each interval is closed on the \n",
    "    left but open on the right. \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out2: Series\n",
    "        contains average hourly memory usage in UTC+8\n",
    "    \"\"\"\n",
    "    df2 = pd.read_csv('mem.csv')\n",
    "    df2\n",
    "\n",
    "    df2['Time'] = pd.to_datetime(df2['Time'], utc=True).dt.tz_convert('Asia/Manila')\n",
    "    \n",
    "    out2 = df2.groupby(pd.Grouper(key='Time', freq='H', closed='left')).mean()['accesslab']\n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T09:43:36.356172Z",
     "start_time": "2021-05-29T09:43:36.315018Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af78a8f4ba8d764f85b629b6ec47aca1",
     "grade": true,
     "grade_id": "cell-2df0e3464012ace8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "hmu = hourly_mem_usage()\n",
    "assert_equal(len(hmu), 49)\n",
    "assert_equal(\n",
    "    hmu.index[:10].tolist(),\n",
    "    [pd.Timestamp('2021-05-28 00:00:00+0800', tz='Asia/Manila', freq='H'),\n",
    "     pd.Timestamp('2021-05-28 01:00:00+0800', tz='Asia/Manila', freq='H'),\n",
    "     pd.Timestamp('2021-05-28 02:00:00+0800', tz='Asia/Manila', freq='H'),\n",
    "     pd.Timestamp('2021-05-28 03:00:00+0800', tz='Asia/Manila', freq='H'),\n",
    "     pd.Timestamp('2021-05-28 04:00:00+0800', tz='Asia/Manila', freq='H'),\n",
    "     pd.Timestamp('2021-05-28 05:00:00+0800', tz='Asia/Manila', freq='H'),\n",
    "     pd.Timestamp('2021-05-28 06:00:00+0800', tz='Asia/Manila', freq='H'),\n",
    "     pd.Timestamp('2021-05-28 07:00:00+0800', tz='Asia/Manila', freq='H'),\n",
    "     pd.Timestamp('2021-05-28 08:00:00+0800', tz='Asia/Manila', freq='H'),\n",
    "     pd.Timestamp('2021-05-28 09:00:00+0800', tz='Asia/Manila', freq='H')]\n",
    ")\n",
    "assert_allclose(\n",
    "    hmu[:10].tolist(),\n",
    "    [369325498368.0,\n",
    "     389165290837.3333,\n",
    "     394183460454.4,\n",
    "     388326653132.8,\n",
    "     390690503065.6,\n",
    "     391548573832.5333,\n",
    "     389338755072.0,\n",
    "     389219362406.4,\n",
    "     390718947328.0,\n",
    "     375491907037.86664]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49d274136c0aeda9d6237c92de495711",
     "grade": false,
     "grade_id": "cell-2444a25a728640d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 3\n",
    "\n",
    "Create a function `daily_mem_usage` that reads `mem.csv` and returns the mean daily memory usage as a pandas time series with a daily period index. The first column of `mem.csv` is the time and the second column is the memory usage in bytes. Each interval is closed on the left but open on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T03:21:33.035073Z",
     "start_time": "2021-05-30T03:21:33.028657Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db5d1bf94191e904b90893c7b7918a57",
     "grade": false,
     "grade_id": "cell-702732e11f1d52be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def daily_mem_usage():\n",
    "    \"\"\"\n",
    "    Compute mean daily memory usage\n",
    "    \n",
    "    Reads mem.csv and returns the mean daily memory usage as a pandas time \n",
    "    series with a daily period index. Each interval is closed on the left\n",
    "    but open on the right.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out3: Series\n",
    "        contains mean daily memory usage with daily period as indices\n",
    "    \"\"\"\n",
    "    df3 = pd.read_csv('mem.csv')\n",
    "    df3['Time'] = pd.to_datetime(df3['Time'])\n",
    "\n",
    "    df3['Time'] = pd.to_datetime(df3['Time'])\n",
    "    df3['date'] = pd.PeriodIndex(df3.Time, freq='D')\n",
    "\n",
    "    out3 = df3.groupby(df3.date)['accesslab'].mean()\n",
    "    return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T03:35:47.099145Z",
     "start_time": "2021-05-30T03:35:47.073184Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68f728ec2f0a9bca24d4aa8e42e55353",
     "grade": true,
     "grade_id": "cell-875a1de1e6697679",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dmu = daily_mem_usage()\n",
    "assert_equal(\n",
    "    dmu.index[:2].tolist(),\n",
    "    [pd.Period('2021-05-27', 'D'), pd.Period('2021-05-28', 'D')]\n",
    ")\n",
    "assert_allclose(\n",
    "    dmu[:2].tolist(),\n",
    "    [390154853588.5283, 346686658616.8889]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a97413d619a6b47c38562f1e42fa3e5e",
     "grade": false,
     "grade_id": "cell-1e930d4d2fb0129d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 4\n",
    "\n",
    "Create a function `longest_distances` that reads the first 1M data lines of `/mnt/data/public/nyctaxi/trip_data/trip_data_1.csv` and returns a `pandas.Series` containing the maximum `trip_distance` for each pickup hour of day and passenger count. The index is a hierarchical index of `pickup_datetime` and `passenger_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T08:08:50.137898Z",
     "start_time": "2021-05-28T08:08:50.133219Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "678c8cc77fad3fc240ec952fee0a1409",
     "grade": false,
     "grade_id": "cell-5f238bcbd72181a3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def longest_distances():\n",
    "    \"\"\"\n",
    "    Get maximum trip_distance values\n",
    "    \n",
    "    Reads the first 1M data lines of filepath '/mnt/data/public/nyctaxi/\n",
    "    trip_data/trip_data_1.csv' and returns a pandas Series containing the \n",
    "    maximum trip_distance for each pickup hour of day and passenger count. \n",
    "    The index is a hierarchical index of pickup_datetime and passenger_count.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out4: Series\n",
    "        contains maximum trip_distance values\n",
    "    \"\"\"\n",
    "    df4 = pd.read_csv('/mnt/data/public/nyctaxi/trip_data/trip_data_1.csv', \n",
    "                      nrows=1000000)\n",
    "    df4.head()\n",
    "    df4['pickup_datetime'] = pd.to_datetime(df4['pickup_datetime'])\n",
    "    out4 = (df4.groupby([df4['pickup_datetime'].dt.hour,df4.passenger_count])\n",
    "            ['trip_distance'].max())\n",
    "    return out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medallion</th>\n",
       "      <th>hack_license</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>rate_code</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_time_in_secs</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89D227B655E5C82AECF13C3F540D4CF4</td>\n",
       "      <td>BA96DE419E711691B9445D6A6307C170</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-01 15:11:48</td>\n",
       "      <td>2013-01-01 15:18:10</td>\n",
       "      <td>4</td>\n",
       "      <td>382</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-73.978165</td>\n",
       "      <td>40.757977</td>\n",
       "      <td>-73.989838</td>\n",
       "      <td>40.751171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n",
       "      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-06 00:18:35</td>\n",
       "      <td>2013-01-06 00:22:54</td>\n",
       "      <td>1</td>\n",
       "      <td>259</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-74.006683</td>\n",
       "      <td>40.731781</td>\n",
       "      <td>-73.994499</td>\n",
       "      <td>40.750660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n",
       "      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-05 18:49:41</td>\n",
       "      <td>2013-01-05 18:54:23</td>\n",
       "      <td>1</td>\n",
       "      <td>282</td>\n",
       "      <td>1.1</td>\n",
       "      <td>-74.004707</td>\n",
       "      <td>40.737770</td>\n",
       "      <td>-74.009834</td>\n",
       "      <td>40.726002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n",
       "      <td>51EE87E3205C985EF8431D850C786310</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-07 23:54:15</td>\n",
       "      <td>2013-01-07 23:58:20</td>\n",
       "      <td>2</td>\n",
       "      <td>244</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-73.974602</td>\n",
       "      <td>40.759945</td>\n",
       "      <td>-73.984734</td>\n",
       "      <td>40.759388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n",
       "      <td>51EE87E3205C985EF8431D850C786310</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-07 23:25:03</td>\n",
       "      <td>2013-01-07 23:34:24</td>\n",
       "      <td>1</td>\n",
       "      <td>560</td>\n",
       "      <td>2.1</td>\n",
       "      <td>-73.976250</td>\n",
       "      <td>40.748528</td>\n",
       "      <td>-74.002586</td>\n",
       "      <td>40.747868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          medallion                      hack_license  \\\n",
       "0  89D227B655E5C82AECF13C3F540D4CF4  BA96DE419E711691B9445D6A6307C170   \n",
       "1  0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
       "2  0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
       "3  DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
       "4  DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
       "\n",
       "  vendor_id  rate_code store_and_fwd_flag      pickup_datetime  \\\n",
       "0       CMT          1                  N  2013-01-01 15:11:48   \n",
       "1       CMT          1                  N  2013-01-06 00:18:35   \n",
       "2       CMT          1                  N  2013-01-05 18:49:41   \n",
       "3       CMT          1                  N  2013-01-07 23:54:15   \n",
       "4       CMT          1                  N  2013-01-07 23:25:03   \n",
       "\n",
       "      dropoff_datetime  passenger_count  trip_time_in_secs  trip_distance  \\\n",
       "0  2013-01-01 15:18:10                4                382            1.0   \n",
       "1  2013-01-06 00:22:54                1                259            1.5   \n",
       "2  2013-01-05 18:54:23                1                282            1.1   \n",
       "3  2013-01-07 23:58:20                2                244            0.7   \n",
       "4  2013-01-07 23:34:24                1                560            2.1   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \n",
       "0        -73.978165        40.757977         -73.989838         40.751171  \n",
       "1        -74.006683        40.731781         -73.994499         40.750660  \n",
       "2        -74.004707        40.737770         -74.009834         40.726002  \n",
       "3        -73.974602        40.759945         -73.984734         40.759388  \n",
       "4        -73.976250        40.748528         -74.002586         40.747868  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T08:08:52.875187Z",
     "start_time": "2021-05-28T08:08:50.140184Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d0576b907efc6bdc295b432959b9bbb",
     "grade": true,
     "grade_id": "cell-edd54e2dfb967fd5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "distances = longest_distances()\n",
    "assert_equal(isinstance(distances, pd.core.series.Series), True)\n",
    "assert_equal(distances.shape, (149,))\n",
    "assert_equal(distances.index.levshape, (24, 7))\n",
    "assert_equal(distances.index.names, ['pickup_datetime', 'passenger_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d4f59af9792800c849fd5de2fadbf44",
     "grade": false,
     "grade_id": "cell-b6c3cb3d9e2d7d3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 5\n",
    "Create a function `mean_ratings` that reads `/mnt/data/public/insideairbnb/data.insideairbnb.com/united-kingdom/england/london/2015-04-06/data/listings.csv.gz` and returns a `pandas.Series` containing the average `review_scores_ratings` of hosts, binned by monthly `host_since`. Ignore `NA`s or null values when computing the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T08:08:52.882968Z",
     "start_time": "2021-05-28T08:08:52.878352Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aafcc7cd6c93c2a252892d727c0b09a6",
     "grade": false,
     "grade_id": "cell-990d37718690cffb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mean_ratings():\n",
    "    \"\"\"\n",
    "    Compute average review_scores_ratings values\n",
    "    \n",
    "    Reads /mnt/data/public/insideairbnb/data.insideairbnb.com/united-kingdom/\n",
    "    england/london/2015-04-06/data/listings.csv.gz and returns a pandas.Series\n",
    "    containing the average review_scores_ratings of hosts, binned by monthly \n",
    "    host_since. Ignore NAs or null values when computing the mean.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Series\n",
    "        contains average review_scores_ratings values\n",
    "    \"\"\"\n",
    "    df5 = pd.read_csv('/mnt/data/public/insideairbnb/data.insideairbnb.com/united'\n",
    "                      '-kingdom/england/london/2015-04-06/data/listings.csv.gz')\n",
    "    df5['host_since'] = pd.to_datetime(df5.host_since)\n",
    "\n",
    "    df5_new = df5[['host_since','review_scores_rating']].copy()\n",
    "    grpby = df5_new.groupby(pd.Grouper(key = 'host_since', freq='M')).mean()\n",
    "\n",
    "    return grpby['review_scores_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T08:08:53.867847Z",
     "start_time": "2021-05-28T08:08:52.884167Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "858aff6938229afb1032540e507da2f8",
     "grade": true,
     "grade_id": "cell-1c32dec1fa166f06",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "ratings = mean_ratings()\n",
    "assert_equal(isinstance(ratings, pd.core.series.Series), True)\n",
    "assert_equal(ratings.shape, (80,))\n",
    "assert_equal(ratings.index[0], datetime.datetime(2008, 9, 30))\n",
    "assert_equal(ratings.index[-1], datetime.datetime(2015, 4, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddf03169abd3837ee1db43c779e82390",
     "grade": false,
     "grade_id": "cell-c7aa9df0e919d86a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 6\n",
    "\n",
    "Create a function `product_aisles` that reads `/mnt/data/public/instacart/instacart_2017_05_01/products.csv` and returns a `pandas.DataFrame`. The data frame should have the contents of that file but with an additional column `aisle` that corresponds to the name of the aisle according to `/mnt/data/public/instacart/instacart_2017_05_01/aisles.csv`. The index should be the `product_id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T08:08:53.872713Z",
     "start_time": "2021-05-28T08:08:53.869737Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fe14609303219a5b27ab1420b8553d4",
     "grade": false,
     "grade_id": "cell-40c2bc95fd97b5af",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def product_aisles():\n",
    "    df6_prod = pd.read_csv('/mnt/data/public/instacart/instacart_2017_05_01'\n",
    "                      '/products.csv')    \n",
    "    df6_aisle = pd.read_csv('/mnt/data/public/instacart/'\n",
    "                            'instacart_2017_05_01/aisles.csv')\n",
    "    df6_prod = df6_prod.set_index('product_id')\n",
    "\n",
    "    out6 = df6_prod.merge(df6_aisle, how = 'left', on ='aisle_id')\n",
    "    return out6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T08:08:53.944142Z",
     "start_time": "2021-05-28T08:08:53.874399Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9830f8a10fe6d493b2e08590d1457d08",
     "grade": true,
     "grade_id": "cell-4e4a5307f3124e7c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_products_aisles = product_aisles()\n",
    "assert_equal(isinstance(df_products_aisles, pd.DataFrame), True)\n",
    "assert_equal(\n",
    "    df_products_aisles.columns.tolist(), \n",
    "    ['product_name', 'aisle_id', 'department_id', 'aisle'])\n",
    "assert_equal(df_products_aisles.shape, (49688, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c61599c67ee7b6cdc16622b12936f8b",
     "grade": false,
     "grade_id": "cell-fbd761467bffbe89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 7\n",
    "\n",
    "Create a function `tracks_with_loc` that reads `/mnt/data/public/millionsong/AdditionalFiles/unique_tracks.txt` and returns a `pandas` `DataFrame` that contains the contents of the file but with the artist location from `/mnt/data/public/millionsong/AdditionalFiles/artist_location.txt` matched based on the artist name. Leave the value as null if the artist is not found in `artist_location.txt`. Use the first entry in the file if an artist has more than one location. Sort the data frame by increasing track ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T08:08:53.950512Z",
     "start_time": "2021-05-28T08:08:53.946653Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56f14e154ce6f0898a9f2fd1909165a9",
     "grade": false,
     "grade_id": "cell-378eb694f1a79212",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tracks_with_loc():\n",
    "    \"\"\"\n",
    "    Perform left join on DataFrames from two text files.\n",
    "    \n",
    "    \n",
    "    Reads /mnt/data/public/millionsong/AdditionalFiles/unique_tracks.txt and \n",
    "    returns a pandas DataFrame that contains the contents of the file but with\n",
    "    the artist location from /mnt/data/public/millionsong/AdditionalFiles/\n",
    "    artist_location.txt matched based on the artist name. Leaves the value as \n",
    "    null if the artist is not found in artist_location.txt. Uses the first \n",
    "    entry in the file if an artist has more than one location. Sorts the data \n",
    "    frame by increasing track ID.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out7: DataFrame\n",
    "        resulting dataframe from left joining dataframes from the two text \n",
    "        files\n",
    "    \"\"\"\n",
    "    df7_UT = pd.read_csv('/mnt/data/public/millionsong/AdditionalFiles/'\n",
    "                      'unique_tracks.txt', sep='<SEP>', engine='python',\n",
    "                        names=['track_id','song_id','artist','title'])\n",
    "    df7_AL = pd.read_csv('/mnt/data/public/millionsong/'\n",
    "                         'AdditionalFiles/artist_location.txt',\n",
    "                         sep='<SEP>', engine='python',\n",
    "                         names=['artist_id','lat','lon','artist','location'])\n",
    "\n",
    "    df7_AL.drop_duplicates(subset='artist', keep='first', inplace = True)\n",
    "    out7 = df7_UT.merge(df7_AL, left_on='artist', right_on='artist', how='left').sort_values(by='track_id')\n",
    "    return out7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T08:09:00.476990Z",
     "start_time": "2021-05-28T08:08:53.952140Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a665c504a5acf1d1a82b639b1f9d56bd",
     "grade": true,
     "grade_id": "cell-74c5a21523df3512",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_tracks_loc = tracks_with_loc()\n",
    "assert_equal(isinstance(df_tracks_loc, pd.DataFrame), True)\n",
    "assert_equal(df_tracks_loc.shape, (1000000, 8))\n",
    "assert_equal(\n",
    "    df_tracks_loc.columns.tolist(), \n",
    "    ['track_id', 'song_id', 'artist', 'title', \n",
    "     'artist_id', 'lat', 'lon', 'location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9dd90ad4d49acbb2484154ae2ad3c410",
     "grade": false,
     "grade_id": "cell-023ea14c96e6474c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 8\n",
    "Create a function `party_votes` that reads `/mnt/data/public/elections/comelec/congress_results/congressional_results_2013.csv` and returns a `pandas` `DataFrame` where the index is the `province_or_city`, the columns are the `party_affiliation` and the values are the total number of votes as integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T08:09:00.482318Z",
     "start_time": "2021-05-28T08:09:00.479005Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3710d7ba5bfcfa98b6c9fb671f784ce1",
     "grade": false,
     "grade_id": "cell-c5bd8a8963f5b048",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def party_votes():\n",
    "    \"\"\"\n",
    "    Perform pivot table on the csv file read\n",
    "    \n",
    "    \n",
    "    Reads /mnt/data/public/elections/comelec/congress_results/congressional_\n",
    "    results_2013.csv and returns a pandas DataFrame where the index is the \n",
    "    province_or_city, the columns are the party_affiliation and the values are\n",
    "    the total number of votes as integer.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    piv8: DataFrame\n",
    "        resulting dataframe from pivot table operation\n",
    "    \"\"\"\n",
    "    df8 = pd.read_csv('/mnt/data/public/elections/comelec/congress_results/'\n",
    "                      'congressional_results_2013.csv')\n",
    "    \n",
    "    piv8 = pd.pivot_table(df8, values='votes_obtained', index='province_or_city', columns='party_affiliation')\n",
    "    piv8 = piv8.fillna(0).astype('int64')\n",
    "    \n",
    "    return piv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>province_or_city</th>\n",
       "      <th>position</th>\n",
       "      <th>name</th>\n",
       "      <th>nickname</th>\n",
       "      <th>party_affiliation</th>\n",
       "      <th>votes_obtained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abra</td>\n",
       "      <td>Representative-Lone District</td>\n",
       "      <td>Bernos, Maria Jocelyn Valera</td>\n",
       "      <td>Joy</td>\n",
       "      <td>Liberal Party</td>\n",
       "      <td>78447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Agusan del Norte</td>\n",
       "      <td>Representative-1st District</td>\n",
       "      <td>Fortun, Lawrence Lemuel Hernandez</td>\n",
       "      <td>Law</td>\n",
       "      <td>Liberal Party</td>\n",
       "      <td>94483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Agusan del Norte</td>\n",
       "      <td>Representative-2nd District</td>\n",
       "      <td>Amante, ErLiberal Partye John Malbas</td>\n",
       "      <td>Ping</td>\n",
       "      <td>KUSGAN</td>\n",
       "      <td>102240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agusan del Sur</td>\n",
       "      <td>Representative-1st District</td>\n",
       "      <td>Plaza, Maria Valentina Galido</td>\n",
       "      <td>Tina</td>\n",
       "      <td>National Unity Party</td>\n",
       "      <td>74537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agusan del Sur</td>\n",
       "      <td>Representative-2nd District</td>\n",
       "      <td>Mellana, Evelyn Plaza</td>\n",
       "      <td>Bebs</td>\n",
       "      <td>National Unity Party</td>\n",
       "      <td>64027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   province_or_city                      position  \\\n",
       "0              Abra  Representative-Lone District   \n",
       "1  Agusan del Norte   Representative-1st District   \n",
       "2  Agusan del Norte   Representative-2nd District   \n",
       "3    Agusan del Sur   Representative-1st District   \n",
       "4    Agusan del Sur   Representative-2nd District   \n",
       "\n",
       "                                   name nickname     party_affiliation  \\\n",
       "0          Bernos, Maria Jocelyn Valera      Joy         Liberal Party   \n",
       "1     Fortun, Lawrence Lemuel Hernandez      Law         Liberal Party   \n",
       "2  Amante, ErLiberal Partye John Malbas     Ping                KUSGAN   \n",
       "3         Plaza, Maria Valentina Galido     Tina  National Unity Party   \n",
       "4                 Mellana, Evelyn Plaza     Bebs  National Unity Party   \n",
       "\n",
       "   votes_obtained  \n",
       "0           78447  \n",
       "1           94483  \n",
       "2          102240  \n",
       "3           74537  \n",
       "4           64027  "
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T08:09:00.511670Z",
     "start_time": "2021-05-28T08:09:00.483707Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28d447638cf5c86b84fc4e50ee25d3c9",
     "grade": true,
     "grade_id": "cell-c609d570f23233cf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_votes = party_votes()\n",
    "assert_equal(isinstance(df_votes, pd.DataFrame), True)\n",
    "assert_equal(df_votes.shape, (108, 24))\n",
    "assert_equal((df_votes.dtypes == int).all(), True)\n",
    "assert_equal(df_votes.index[0], 'Abra')\n",
    "assert_equal(df_votes.columns[0], 'BPP')\n",
    "assert_equal(df_votes.iloc[0,0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7d7dc91132d96781c279c02b72bef52",
     "grade": false,
     "grade_id": "cell-ac495330205fc00b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 9\n",
    "Create a function `naia_traffic` that reads `/mnt/data/public/opendata/transport/caap-aircraft/airdata_aircraft_movement_2016.csv` and returns the number of passengers per month, as integer, for every `airline_operator` in `NAIA` as a `pandas` `DataFrame`. The months should be in title case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T08:09:00.517014Z",
     "start_time": "2021-05-28T08:09:00.513258Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0b2c5be79f5bb9b05636957dea41f06",
     "grade": false,
     "grade_id": "cell-27d7c6647b2e5fc3",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def naia_traffic():\n",
    "    \"\"\"\n",
    "    Perform pandas melt function on specified csv file\n",
    "    \n",
    "    \n",
    "    Reads /mnt/data/public/opendata/transport/caap-aircraft/\n",
    "    airdata_aircraft_movement_2016.csv and returns the number of passengers \n",
    "    per month, as integer, for every airline_operator in NAIA as a pandas \n",
    "    DataFrame. The months are in title case.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    melted: DataFrame\n",
    "        resulting dataframe from applying pandas melt function\n",
    "    \"\"\"\n",
    "    df9 = pd.read_csv('/mnt/data/public/opendata/transport/caap-aircraft/'\n",
    "                      'airdata_aircraft_movement_2016.csv')\n",
    "    df9_naia = df9[df9['airport']== 'NAIA']\n",
    "    months = df9_naia.columns[3:-2]\n",
    "    melted = pd.melt(df9_naia, id_vars=['airline_operator'], value_vars=months, var_name='month', value_name='passengers')\n",
    "\n",
    "    melted['month'] = melted['month'].str.title()\n",
    "    melted['passengers'] = melted['passengers'].astype('int64')\n",
    "\n",
    "    return melted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T08:09:00.538612Z",
     "start_time": "2021-05-28T08:09:00.518222Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "990973526d80d29c11fdcf4811533f13",
     "grade": true,
     "grade_id": "cell-c75134a88e986f33",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_naia = naia_traffic()\n",
    "assert_equal(isinstance(df_naia, pd.DataFrame), True)\n",
    "assert_equal(\n",
    "    df_naia.columns.tolist(), \n",
    "    ['airline_operator', 'month', 'passengers'])\n",
    "assert_equal(df_naia.shape, (36, 3))\n",
    "df_naia_list = df_naia.to_numpy().tolist()\n",
    "assert_equal(['Domestic', 'April', 13517] in df_naia_list, True)\n",
    "assert_equal(['G. Aviation', 'April', 3586] in df_naia_list, True)\n",
    "assert_equal(['International', 'April', 8587] in df_naia_list, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d25efed198aa3056f0408fe0ef127313",
     "grade": false,
     "grade_id": "cell-7220c2a9919a7cba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 10\n",
    "\n",
    "Create a function `pudo` that reads the first 1M data lines of `/mnt/data/public/nyctaxi/all/yellow_tripdata_2017-12.csv` and returns a `pandas` `DataFrame` where the index is the unique values of `PULocationID`, the columns are the unique values of `DOLocationID`, and the values are the number of times the `PULocationID`-`DOLocationID` pair occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T08:09:00.543514Z",
     "start_time": "2021-05-28T08:09:00.540198Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "effabf30a40ab342a46a7ba1985c7ce7",
     "grade": false,
     "grade_id": "cell-5107e45cf1b18dd9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pudo():\n",
    "    \"\"\"\n",
    "    Display the count of unique value pairs as dataframe values\n",
    "    \n",
    "    \n",
    "    Reads the first 1M data lines of /mnt/data/public/nyctaxi/all/\n",
    "    yellow_tripdata_2017-12.csv and returns a pandas DataFrame where the index\n",
    "    is the unique values of PULocationID, the columns are the unique values of\n",
    "    DOLocationID, and the values are the number of times the \n",
    "    PULocationID-DOLocationID pair occurred.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pivot10: DataFrame\n",
    "        resulting dataframe\n",
    "    \"\"\"\n",
    "    df10 = pd.read_csv('/mnt/data/public/nyctaxi/all/yellow_tripdata_2017-12.csv', nrows=1000000)\n",
    "    df10.head()\n",
    "    pivot10 = pd.pivot_table(df10, index=['PULocationID'], values='RatecodeID', columns=['DOLocationID'], aggfunc=len, fill_value=0)\n",
    "\n",
    "    return pivot10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T08:09:01.472144Z",
     "start_time": "2021-05-28T08:09:00.545095Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9087cd2661aba18bc040e7b6ac5a48a9",
     "grade": true,
     "grade_id": "cell-b20289a550a9cad2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_pudo = pudo()\n",
    "assert_equal(df_pudo.shape, (243, 261))\n",
    "assert_equal(df_pudo.index[:10].tolist(), [1, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
    "nonzeros = df_pudo.to_numpy().nonzero()\n",
    "assert_equal(nonzeros[0][:10], [0, 0, 0, 1, 1, 1, 1, 1, 1, 2])\n",
    "assert_equal(nonzeros[1][:10], [0, 128, 260, 2, 31, 68, 133, 155, 164, 0])\n",
    "assert_equal(\n",
    "    df_pudo.iloc[0][:10].to_numpy(), \n",
    "    [86,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
